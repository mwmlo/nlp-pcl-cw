{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP CW - Michelle Lo, Hetty Symes, Evelyn Nutton\n",
    "\n",
    "RoBERTa base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel, AutoTokenizer\n",
    "import nltk\n",
    "from dataset.dont_patronize_me import DontPatronizeMe\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df = pd.read_csv(\"train_dev_data/train_set.csv\")\n",
    "test_df = pd.read_csv(\"train_dev_data/dev_set.csv\")\n",
    "dontpatroniseme = DontPatronizeMe(None, \"test_data/task4_test.tsv\")\n",
    "dontpatroniseme.load_test()\n",
    "official_test_df =  dontpatroniseme.test_set_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Resampled and Augmented Data into New Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-sampled, pre-augmented dataset\n",
    "train_df = pd.read_csv(\"train_dev_data/train_set_aug_resampled.csv\")\n",
    "\n",
    "# Verify the oversampling result\n",
    "print(train_df['label'].value_counts())\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Roberta Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "checkpoint = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, truncation=True, do_lower_case=True)\n",
    "pretrained_model = RobertaModel.from_pretrained(checkpoint, num_labels=2)\n",
    "pretrained_model = pretrained_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCLData class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for the data\n",
    "class PCLData(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len, test=False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.text = self.data.text\n",
    "        self.test = test\n",
    "        self.targets = None if test else self.data.label\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor([]) if self.test else torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TRAIN Dataset: {}\".format(train_df.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_df.shape))\n",
    "train_dataset = PCLData(train_df, tokenizer, MAX_LEN)\n",
    "test_dataset = PCLData(test_df, tokenizer, MAX_LEN)\n",
    "\n",
    "test_params = {'batch_size': 4, 'shuffle': True, 'num_workers': 0}\n",
    "testing_loader = DataLoader(test_dataset, **test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobertaClass, self).__init__()\n",
    "        self.l1 = pretrained_model\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Loss - Self Adjusting Dice Loss\n",
    "Taken from the unofficial Pytorch implementation of https://arxiv.org/abs/1911.02855, which can be founds here https://github.com/fursovia/self-adj-dice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "\n",
    "# Taken from the SelfAdjDiceLoss python module source code which cannot be imported regularly due to pytorch compatibility issues\n",
    "class SelfAdjDiceLoss(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    Creates a criterion that optimizes a multi-class Self-adjusting Dice Loss\n",
    "    (\"Dice Loss for Data-imbalanced NLP Tasks\" paper)\n",
    "\n",
    "    Args:\n",
    "        alpha (float): a factor to push down the weight of easy examples\n",
    "        gamma (float): a factor added to both the nominator and the denominator for smoothing purposes\n",
    "        reduction (string): Specifies the reduction to apply to the output:\n",
    "            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
    "            ``'mean'``: the sum of the output will be divided by the number of\n",
    "            elements in the output, ``'sum'``: the output will be summed.\n",
    "\n",
    "    Shape:\n",
    "        - logits: `(N, C)` where `N` is the batch size and `C` is the number of classes.\n",
    "        - targets: `(N)` where each value is in [0, C - 1]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha: float = 1.0, gamma: float = 1.0, reduction: str = \"mean\") -> None:\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        print(logits.shape)\n",
    "        probs = torch.gather(probs, dim=1, index=targets.unsqueeze(1))\n",
    "\n",
    "        probs_with_factor = ((1 - probs) ** self.alpha) * probs\n",
    "        loss = 1 - (2 * probs_with_factor + self.gamma) / (probs_with_factor + 1 + self.gamma)\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        elif self.reduction == \"none\" or self.reduction is None:\n",
    "            return loss\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Reduction `{self.reduction}` is not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating the loss function and optimizer\n",
    "# criterion = SelfAdjDiceLoss()\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.AdamW(params =  model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def calcuate_accuracy(preds, targets):\n",
    "    n_correct = (preds==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, epoch, optimizer, training_loader, scheduler=None):\n",
    "    tr_loss = 0; n_correct = 0; steps = 0; seen = 0\n",
    "    model.train()\n",
    "    for i,data in tqdm(enumerate(training_loader, 0)):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        preds = model(ids, mask, token_type_ids)\n",
    "        loss = loss_function(preds, targets)\n",
    "        tr_loss += loss.item()\n",
    "        _, pred_labels = torch.max(preds.data, dim=1)\n",
    "        n_correct += calcuate_accuracy(pred_labels, targets)\n",
    "\n",
    "        steps += 1\n",
    "        seen+=targets.size(0)\n",
    "        \n",
    "        if i%5000==0:\n",
    "            curr_loss = tr_loss/steps\n",
    "            curr_acc = (n_correct*100)/seen \n",
    "            print(f\"Training Loss per 5000 steps: {curr_loss}\")\n",
    "            print(f\"Training Accuracy per 5000 steps: {curr_acc}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "    print(f'Total Accuracy for Epoch {epoch}: {(n_correct*100)/seen}')\n",
    "    epoch_loss = tr_loss/steps\n",
    "    epoch_accu = (n_correct*100)/seen\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    model.eval()\n",
    "    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; steps=0; seen=0\n",
    "    preds_model = torch.tensor([]).to(device); targets_model = torch.tensor([]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            preds = model(ids, mask, token_type_ids).squeeze()\n",
    "            \n",
    "            _, pred_labels = torch.max(preds.data, dim=1)\n",
    "            n_correct += calcuate_accuracy(pred_labels, targets)\n",
    "\n",
    "            steps += 1\n",
    "            seen+=targets.size(0)\n",
    "\n",
    "            preds_model = torch.cat((preds_model, pred_labels))\n",
    "            targets_model = torch.cat((targets_model, targets))\n",
    "            \n",
    "    epoch_accu = (n_correct*100)/seen\n",
    "\n",
    "    \n",
    "    \n",
    "    return epoch_accu, preds_model, targets_model\n",
    "\n",
    "# acc, preds, targets = valid(model, testing_loader)\n",
    "# print(\"Accuracy on test data = %0.2f%%\" % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_hyperparameters(save_model_name, learning_rate, batch_size, epochs, use_scheduler=False, gamma=0.9):\n",
    "    train_params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 0}\n",
    "    training_loader = DataLoader(train_dataset, **train_params)\n",
    "    model = RobertaClass().to(device)\n",
    "    optimizer = torch.optim.AdamW(params =  model.parameters(), lr=learning_rate)\n",
    "\n",
    "    scheduler = None\n",
    "    if use_scheduler:\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma)\n",
    "    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train(model, epoch, optimizer, training_loader, scheduler)\n",
    "\n",
    "    torch.save(model.state_dict(), f\"models/{save_model_name}.pt\")\n",
    "\n",
    "    acc, preds, targets = valid(model, testing_loader)\n",
    "    print(\"Accuracy on test data = %0.2f%%\" % acc)\n",
    "    print(classification_report(targets.cpu().numpy(), preds.cpu().numpy()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters to tune: learning rate and batch size\n",
    "\n",
    "batch_sizes = [4, 16, 32]\n",
    "learning_rates = [1e-5, 1e-3, 1e-2]\n",
    "gamma_rates = [0.3, 0.5, 0.9]\n",
    "\n",
    "model_id = 0\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for lr in learning_rates:\n",
    "        print(f\"Model {model_id}: Batch size {batch_size}, LR {lr}, no scheduler\")\n",
    "        train_with_hyperparameters(model_id, lr, batch_size, 5, use_scheduler=False)\n",
    "        model_id += 1\n",
    "        for gamma in gamma_rates:\n",
    "            print(f\"Model {model_id}: Batch size {batch_size}, LR {lr}, scheduler with gamma {gamma}\")\n",
    "            train_with_hyperparameters(model_id, lr, batch_size, 5, use_scheduler=True, gamma=gamma)\n",
    "            model_id += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
