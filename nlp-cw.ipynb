{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP CW - Michelle Lo, Hetty Symes, Evelyn Nutton\n",
    "\n",
    "RoBERTa base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /homes/en120/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /homes/en120/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import pipeline, RobertaModel, AutoTokenizer, AutoModelForSequenceClassification, AdamW, DataCollatorWithPadding, get_scheduler\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "import nlpaug.augmenter.word as naw\n",
    "import sacremoses\n",
    "import nltk\n",
    "import math\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df = pd.read_csv(\"train_dev_data/train_set.csv\")\n",
    "test_df = pd.read_csv(\"train_dev_data/dev_set.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation via Back Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m underrep \u001b[38;5;241m=\u001b[39m train_df[train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      7\u001b[0m underrep_augment \u001b[38;5;241m=\u001b[39m underrep\u001b[38;5;241m.\u001b[39mcopy()\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[0;32m----> 8\u001b[0m underrep_augment_data \u001b[38;5;241m=\u001b[39m \u001b[43mback_aug\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m(\u001b[49m\u001b[43munderrep_augment\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m underrep_augment_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m underrep_augment_data \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(t) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mfloat\u001b[39m])\n\u001b[1;32m     10\u001b[0m underrep_augment[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(underrep_augment_data)\n",
      "File \u001b[0;32m/vol/bitbucket/en120/dlenv/lib/python3.12/site-packages/nlpaug/base_augmenter.py:98\u001b[0m, in \u001b[0;36mAugmenter.augment\u001b[0;34m(self, data, n, num_thread)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAbstSummAug\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBackTranslationAug\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContextualWordEmbsAug\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContextualWordEmbsForSentenceAug\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(aug_num):\n\u001b[0;32m---> 98\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43maction_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    100\u001b[0m             augmented_results\u001b[38;5;241m.\u001b[39mextend(result)\n",
      "File \u001b[0;32m/vol/bitbucket/en120/dlenv/lib/python3.12/site-packages/nlpaug/augmenter/word/back_translation.py:70\u001b[0m, in \u001b[0;36mBackTranslationAug.substitute\u001b[0;34m(self, data, n)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[0;32m---> 70\u001b[0m augmented_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m augmented_text\n",
      "File \u001b[0;32m/vol/bitbucket/en120/dlenv/lib/python3.12/site-packages/nlpaug/model/lang_models/machine_translation_transformers.py:40\u001b[0m, in \u001b[0;36mMtTransformers.predict\u001b[0;34m(self, texts, target_words, n)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts, target_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     39\u001b[0m     src_translated_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranslate_one_step_batched(texts, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_tokenizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_model)\n\u001b[0;32m---> 40\u001b[0m     tgt_translated_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate_one_step_batched\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_translated_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtgt_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtgt_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tgt_translated_texts\n",
      "File \u001b[0;32m/vol/bitbucket/en120/dlenv/lib/python3.12/site-packages/nlpaug/model/lang_models/machine_translation_transformers.py:62\u001b[0m, in \u001b[0;36mMtTransformers.translate_one_step_batched\u001b[0;34m(self, data, tokenizer, model)\u001b[0m\n\u001b[1;32m     59\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(t\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m batch)\n\u001b[1;32m     60\u001b[0m         input_ids, attention_mask \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m---> 62\u001b[0m         translated_ids_batch \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m         all_translated_ids\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     68\u001b[0m             translated_ids_batch\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     69\u001b[0m         )\n\u001b[1;32m     71\u001b[0m all_translated_texts \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/vol/bitbucket/en120/dlenv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/en120/dlenv/lib/python3.12/site-packages/transformers/generation/utils.py:2286\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2278\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2279\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2280\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2281\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2282\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2283\u001b[0m     )\n\u001b[1;32m   2285\u001b[0m     \u001b[38;5;66;03m# 13. run beam sample\u001b[39;00m\n\u001b[0;32m-> 2286\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2287\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2292\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2293\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2294\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[1;32m   2297\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2298\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2299\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2300\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2306\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2307\u001b[0m     )\n",
      "File \u001b[0;32m/vol/bitbucket/en120/dlenv/lib/python3.12/site-packages/transformers/generation/utils.py:3574\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3571\u001b[0m next_tokens \u001b[38;5;241m=\u001b[39m next_tokens \u001b[38;5;241m%\u001b[39m vocab_size\n\u001b[1;32m   3573\u001b[0m \u001b[38;5;66;03m# stateless\u001b[39;00m\n\u001b[0;32m-> 3574\u001b[0m beam_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mbeam_scorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3575\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnext_token_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnext_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnext_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3580\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeam_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeam_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_prompt_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_prompt_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3585\u001b[0m beam_scores \u001b[38;5;241m=\u001b[39m beam_outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_beam_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   3586\u001b[0m beam_next_tokens \u001b[38;5;241m=\u001b[39m beam_outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_beam_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/vol/bitbucket/en120/dlenv/lib/python3.12/site-packages/transformers/generation/beam_search.py:273\u001b[0m, in \u001b[0;36mBeamSearchScorer.process\u001b[0;34m(self, input_ids, next_scores, next_tokens, next_indices, pad_token_id, eos_token_id, beam_indices, group_index, decoder_prompt_len)\u001b[0m\n\u001b[1;32m    271\u001b[0m batch_beam_idx \u001b[38;5;241m=\u001b[39m batch_idx \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_size \u001b[38;5;241m+\u001b[39m next_index\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# add to generated hypotheses if end of sentence\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (eos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[43mnext_token\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01min\u001b[39;00m eos_token_id):\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;66;03m# if beam_token does not belong to top num_beams tokens, it should not be added\u001b[39;00m\n\u001b[1;32m    275\u001b[0m     is_beam_token_worse_than_top_num_beams \u001b[38;5;241m=\u001b[39m beam_token_rank \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_size\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_beam_token_worse_than_top_num_beams:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "back_aug = naw.BackTranslationAug(\n",
    "    from_model_name='Helsinki-NLP/opus-mt-en-de',\n",
    "    to_model_name='Helsinki-NLP/opus-mt-de-en',\n",
    "    device=device\n",
    ")\n",
    "underrep = train_df[train_df['label'] == 1]\n",
    "underrep_augment = underrep.copy().dropna()\n",
    "underrep_augment_data = back_aug.augment(underrep_augment[\"text\"].to_list())\n",
    "underrep_augment_data = pd.DataFrame([t for t in underrep_augment_data if type(t) != float])\n",
    "underrep_augment[[\"text\"]] = pd.DataFrame(underrep_augment_data)\n",
    "print(underrep[\"text\"].to_list()[0])\n",
    "print(underrep_augment_data.to_list()[0])\n",
    "train_df_augment = pd.concat([train_df,underrep_augment])\n",
    "\n",
    "print(train_df['label'].value_counts())\n",
    "print(train_df_augment['label'].value_counts())\n",
    "\n",
    "train_df = train_df_augment\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  We 're living in times of absolute insanity , ...      0\n",
      "1  In Libya today , there are countless number of...      0\n",
      "2  \"White House press secretary Sean Spicer said ...      0\n",
      "3  Council customers only signs would be displaye...      0\n",
      "4  \"\"\" Just like we received migrants fleeing El ...      0\n",
      "label\n",
      "0    7581\n",
      "1    7581\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We 're living in times of absolute insanity , ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In Libya today , there are countless number of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"White House press secretary Sean Spicer said ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Council customers only signs would be displaye...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"\"\" Just like we received migrants fleeing El ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  We 're living in times of absolute insanity , ...      0\n",
       "1  In Libya today , there are countless number of...      0\n",
       "2  \"White House press secretary Sean Spicer said ...      0\n",
       "3  Council customers only signs would be displaye...      0\n",
       "4  \"\"\" Just like we received migrants fleeing El ...      0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_df['label'].value_counts())\n",
    "\n",
    "X_train = train_df[['text']]  # Feature columns\n",
    "y_train = train_df['label']  # Target column\n",
    "\n",
    "# Initialize the random oversampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "# Apply oversampling\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Resampled and Augmented Data into New Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    7581\n",
      "1    7581\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We 're living in times of absolute insanity , ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In Libya today , there are countless number of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"White House press secretary Sean Spicer said ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Council customers only signs would be displaye...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"\"\" Just like we received migrants fleeing El ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  We 're living in times of absolute insanity , ...      0\n",
       "1  In Libya today , there are countless number of...      0\n",
       "2  \"White House press secretary Sean Spicer said ...      0\n",
       "3  Council customers only signs would be displaye...      0\n",
       "4  \"\"\" Just like we received migrants fleeing El ...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update the dataset with the resampled values\n",
    "train_df = pd.DataFrame(X_resampled, columns=X_train.columns)\n",
    "train_df['label'] = y_resampled\n",
    "\n",
    "\n",
    "# Verify the oversampling result\n",
    "print(train_df['label'].value_counts())\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Roberta Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model\n",
    "checkpoint = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, truncation=True, do_lower_case=True)\n",
    "pretrained_model = RobertaModel.from_pretrained(checkpoint, num_labels=2)\n",
    "pretrained_model = pretrained_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCLData class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for the data\n",
    "class PCLData(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.text = self.data.text\n",
    "        self.targets = self.data.label\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256\n",
    "# LEARNING_RATE = 1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Dataset: (15162, 2)\n",
      "TEST Dataset: (2094, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAIN Dataset: {}\".format(train_df.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_df.shape))\n",
    "train_dataset = PCLData(train_df, tokenizer, MAX_LEN)\n",
    "test_dataset = PCLData(test_df, tokenizer, MAX_LEN)\n",
    "\n",
    "test_params = {'batch_size': 4, 'shuffle': True, 'num_workers': 0}\n",
    "testing_loader = DataLoader(test_dataset, **test_params)\n",
    "\n",
    "# train_params = {'batch_size': 16,\n",
    "#                 'shuffle': True,\n",
    "#                 'num_workers': 0\n",
    "#                 }\n",
    "\n",
    "# test_params = {'batch_size': 4,\n",
    "#                 'shuffle': True,\n",
    "#                 'num_workers': 0\n",
    "#                 }\n",
    "\n",
    "# training_loader = DataLoader(train_dataset, **train_params)\n",
    "# testing_loader = DataLoader(test_dataset, **test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobertaClass, self).__init__()\n",
    "        self.l1 = pretrained_model\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaClass(\n",
       "  (l1): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RobertaClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Loss - Self Adjusting Dice Loss\n",
    "Taken from the unofficial Pytorch implementation of https://arxiv.org/abs/1911.02855, which can be founds here https://github.com/fursovia/self-adj-dice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "\n",
    "# Taken from the SelfAdjDiceLoss python module source code which cannot be imported regularly due to pytorch compatibility issues\n",
    "class SelfAdjDiceLoss(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    Creates a criterion that optimizes a multi-class Self-adjusting Dice Loss\n",
    "    (\"Dice Loss for Data-imbalanced NLP Tasks\" paper)\n",
    "\n",
    "    Args:\n",
    "        alpha (float): a factor to push down the weight of easy examples\n",
    "        gamma (float): a factor added to both the nominator and the denominator for smoothing purposes\n",
    "        reduction (string): Specifies the reduction to apply to the output:\n",
    "            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
    "            ``'mean'``: the sum of the output will be divided by the number of\n",
    "            elements in the output, ``'sum'``: the output will be summed.\n",
    "\n",
    "    Shape:\n",
    "        - logits: `(N, C)` where `N` is the batch size and `C` is the number of classes.\n",
    "        - targets: `(N)` where each value is in [0, C - 1]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha: float = 1.0, gamma: float = 1.0, reduction: str = \"mean\") -> None:\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        print(logits.shape)\n",
    "        probs = torch.gather(probs, dim=1, index=targets.unsqueeze(1))\n",
    "\n",
    "        probs_with_factor = ((1 - probs) ** self.alpha) * probs\n",
    "        loss = 1 - (2 * probs_with_factor + self.gamma) / (probs_with_factor + 1 + self.gamma)\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        elif self.reduction == \"none\" or self.reduction is None:\n",
    "            return loss\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Reduction `{self.reduction}` is not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating the loss function and optimizer\n",
    "criterion = SelfAdjDiceLoss()\n",
    "# loss_function = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.AdamW(params =  model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def calcuate_accuracy(preds, targets):\n",
    "    n_correct = (preds==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, epoch, optimizer, training_loader, scheduler=None):\n",
    "    tr_loss = 0; n_correct = 0; steps = 0; seen = 0\n",
    "    model.train()\n",
    "    for i,data in tqdm(enumerate(training_loader, 0)):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        preds = model(ids, mask, token_type_ids)\n",
    "        loss = loss_function(preds, targets)\n",
    "        tr_loss += loss.item()\n",
    "        _, pred_labels = torch.max(preds.data, dim=1)\n",
    "        n_correct += calcuate_accuracy(pred_labels, targets)\n",
    "\n",
    "        steps += 1\n",
    "        seen+=targets.size(0)\n",
    "        \n",
    "        if i%5000==0:\n",
    "            curr_loss = tr_loss/steps\n",
    "            curr_acc = (n_correct*100)/seen \n",
    "            print(f\"Training Loss per 5000 steps: {curr_loss}\")\n",
    "            print(f\"Training Accuracy per 5000 steps: {curr_acc}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "    print(f'Total Accuracy for Epoch {epoch}: {(n_correct*100)/seen}')\n",
    "    epoch_loss = tr_loss/steps\n",
    "    epoch_accu = (n_correct*100)/seen\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/vol/bitbucket/mwl21/nlp-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2690: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "524it [00:18, 28.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data = 27.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def valid(model, testing_loader):\n",
    "    model.eval()\n",
    "    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; steps=0; seen=0\n",
    "    preds_model = torch.tensor([]).to(device); targets_model = torch.tensor([]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            preds = model(ids, mask, token_type_ids).squeeze()\n",
    "            \n",
    "            _, pred_labels = torch.max(preds.data, dim=1)\n",
    "            n_correct += calcuate_accuracy(pred_labels, targets)\n",
    "\n",
    "            steps += 1\n",
    "            seen+=targets.size(0)\n",
    "\n",
    "            preds_model = torch.cat((preds_model, pred_labels))\n",
    "            targets_model = torch.cat((targets_model, targets))\n",
    "            \n",
    "    epoch_accu = (n_correct*100)/seen\n",
    "\n",
    "    \n",
    "    \n",
    "    return epoch_accu, preds_model, targets_model\n",
    "\n",
    "acc, preds, targets = valid(model, testing_loader)\n",
    "print(\"Accuracy on test data = %0.2f%%\" % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_hyperparameters(learning_rate, batch_size, epochs, use_scheduler=True, gamma=0.9):\n",
    "    train_params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 0}\n",
    "    training_loader = DataLoader(train_dataset, **train_params)\n",
    "    model = RobertaClass().to(device)\n",
    "    optimizer = torch.optim.AdamW(params =  model.parameters(), lr=learning_rate)\n",
    "\n",
    "    scheduler = None\n",
    "    if use_scheduler:\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma)\n",
    "    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train(model, epoch, optimizer, training_loader, scheduler)\n",
    "\n",
    "    acc, preds, targets = valid(model, testing_loader)\n",
    "    print(\"Accuracy on test data = %0.2f%%\" % acc)\n",
    "    print(classification_report(targets.cpu().numpy(), preds.cpu().numpy()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters to tune: learning rate and batch size\n",
    "\n",
    "batch_sizes = [4, 16, 32]\n",
    "learning_rates = [1e-5, 1e-3, 1e-2]\n",
    "gamma_rates = [0.3, 0.5, 0.9]\n",
    "\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for lr in learning_rates:\n",
    "        print(f\"Batch size {batch_size}, LR {lr}, no scheduler\")\n",
    "        train_with_hyperparameters(lr, batch_size, 5, use_scheduler=False)\n",
    "        for gamma in gamma_rates:\n",
    "            print(f\"Batch size {batch_size}, LR {lr}, scheduler with gamma {gamma}\")\n",
    "            train_with_hyperparameters(lr, batch_size, 5, use_scheduler=True, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/vol/bitbucket/en120/dlenv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2681: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "1it [00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.19256092607975006\n",
      "Training Accuracy per 5000 steps: 93.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "948it [18:48,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Accuracy for Epoch 0: 97.77733808204722\n",
      "Training Loss Epoch: 0.06323339307289237\n",
      "Training Accuracy Epoch: 97.77733808204722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.013898417353630066\n",
      "Training Accuracy per 5000 steps: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "948it [10:03,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Accuracy for Epoch 1: 99.16897506925208\n",
      "Training Loss Epoch: 0.02566149712084986\n",
      "Training Accuracy Epoch: 99.16897506925208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "FINAL_LR = 1e-4\n",
    "FINAL_TRAIN_BATCH_SIZE = 16\n",
    "FINAL_GAMMA = 0.9\n",
    "\n",
    "train_with_hyperparameters(FINAL_LR, FINAL_TRAIN_BATCH_SIZE, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.97      0.96      1895\n",
      "         1.0       0.63      0.55      0.58       199\n",
      "\n",
      "    accuracy                           0.93      2094\n",
      "   macro avg       0.79      0.76      0.77      2094\n",
      "weighted avg       0.92      0.93      0.92      2094\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1830   65]\n",
      " [  90  109]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiQAAAHHCAYAAACPy0PBAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAVRFJREFUeJzt3X1cjff/B/DXSXVK90XloMRI0Rg2cm+aUG7bd5pGyM0oRjRC7oYmm5vctbuvzLQZxtyMaRoNoUVq7m+iGaeQaqFT6vr94df57ig7xbm6cryee1yPh/P5fK7PeV/Hjt59bq5LJgiCACIiIiIJGUgdABERERETEiIiIpIcExIiIiKSHBMSIiIikhwTEiIiIpIcExIiIiKSHBMSIiIikhwTEiIiIpIcExIiIiKSHBMSIhFdunQJvXr1gpWVFWQyGXbs2KHT/q9duwaZTIbY2Fid9vsi6969O7p37y51GERURUxISO9duXIF48aNQ+PGjWFiYgJLS0t06tQJK1euxMOHD0V978DAQKSnp2PRokXYuHEj2rVrJ+r7VacRI0ZAJpPB0tKyws/x0qVLkMlkkMlk+OSTT6rc/82bNzFv3jykpqbqIFoiqukMpQ6ASEx79uzBf/7zH8jlcgwfPhwtW7ZEUVERDh8+jLCwMJw5cwaff/65KO/98OFDJCUlYdasWQgJCRHlPZydnfHw4UMYGRmJ0r82hoaGePDgAXbt2oV33nlHo27Tpk0wMTFBYWHhM/V98+ZNzJ8/H40aNULr1q0rfd7+/fuf6f2ISFpMSEhvZWRkwN/fH87OzkhISEC9evXUdcHBwbh8+TL27Nkj2vvfvn0bAGBtbS3ae8hkMpiYmIjWvzZyuRydOnXCt99+Wy4hiYuLg4+PD7Zt21YtsTx48AC1a9eGsbFxtbwfEekWp2xIb0VFRaGgoABfffWVRjJS5pVXXsEHH3ygfv3o0SN89NFHaNKkCeRyORo1aoSZM2dCpVJpnNeoUSP4+vri8OHDeOONN2BiYoLGjRvj66+/VreZN28enJ2dAQBhYWGQyWRo1KgRgMdTHWV//qd58+ZBJpNplMXHx6Nz586wtraGubk5XF1dMXPmTHX909aQJCQkoEuXLjAzM4O1tTUGDBiAc+fOVfh+ly9fxogRI2BtbQ0rKyuMHDkSDx48ePoH+4ShQ4di7969yM3NVZclJyfj0qVLGDp0aLn2OTk5mDZtGjw8PGBubg5LS0v06dMHp0+fVrc5ePAgXn/9dQDAyJEj1VM/ZdfZvXt3tGzZEikpKejatStq166t/lyeXEMSGBgIExOTctfv7e0NGxsb3Lx5s9LXSkTiYUJCemvXrl1o3LgxOnbsWKn2o0ePxpw5c9CmTRssX74c3bp1Q2RkJPz9/cu1vXz5Mt5++2289dZb+PTTT2FjY4MRI0bgzJkzAIDBgwdj+fLlAIB3330XGzduxIoVK6oU/5kzZ+Dr6wuVSoUFCxbg008/Rf/+/XHkyJF/Pe+XX36Bt7c3srOzMW/ePISGhuLo0aPo1KkTrl27Vq79O++8g7///huRkZF45513EBsbi/nz51c6zsGDB0Mmk+GHH35Ql8XFxaF58+Zo06ZNufZXr17Fjh074Ovri2XLliEsLAzp6eno1q2bOjlwc3PDggULAABjx47Fxo0bsXHjRnTt2lXdz927d9GnTx+0bt0aK1asQI8ePSqMb+XKlahbty4CAwNRUlICAPjss8+wf/9+rFq1CgqFotLXSkQiEoj0UF5engBAGDBgQKXap6amCgCE0aNHa5RPmzZNACAkJCSoy5ydnQUAQmJiorosOztbkMvlwtSpU9VlGRkZAgBh6dKlGn0GBgYKzs7O5WKYO3eu8M+v5PLlywUAwu3bt58ad9l7rF+/Xl3WunVrwd7eXrh796667PTp04KBgYEwfPjwcu83atQojT4HDRok2NnZPfU9/3kdZmZmgiAIwttvvy307NlTEARBKCkpERwdHYX58+dX+BkUFhYKJSUl5a5DLpcLCxYsUJclJyeXu7Yy3bp1EwAIMTExFdZ169ZNo+znn38WAAgLFy4Url69KpibmwsDBw7Ueo1EVH04QkJ6KT8/HwBgYWFRqfY//fQTACA0NFSjfOrUqQBQbq2Ju7s7unTpon5dt25duLq64urVq88c85PK1p78+OOPKC0trdQ5t27dQmpqKkaMGAFbW1t1+auvvoq33npLfZ3/9P7772u87tKlC+7evav+DCtj6NChOHjwIJRKJRISEqBUKiucrgEerzsxMHj8T09JSQnu3r2rno46efJkpd9TLpdj5MiRlWrbq1cvjBs3DgsWLMDgwYNhYmKCzz77rNLvRUTiY0JCesnS0hIA8Pfff1eq/fXr12FgYIBXXnlFo9zR0RHW1ta4fv26RrmTk1O5PmxsbHDv3r1njLi8IUOGoFOnThg9ejQcHBzg7++P77///l+Tk7I4XV1dy9W5ubnhzp07uH//vkb5k9diY2MDAFW6lr59+8LCwgKbN2/Gpk2b8Prrr5f7LMuUlpZi+fLlaNq0KeRyOerUqYO6desiLS0NeXl5lX7P+vXrV2kB6yeffAJbW1ukpqYiOjoa9vb2lT6XiMTHhIT0kqWlJRQKBf74448qnffkotKnqVWrVoXlgiA883uUrW8oY2pqisTERPzyyy8YNmwY0tLSMGTIELz11lvl2j6P57mWMnK5HIMHD8aGDRuwffv2p46OAMDixYsRGhqKrl274ptvvsHPP/+M+Ph4tGjRotIjQcDjz6cqTp06hezsbABAenp6lc4lIvExISG95evriytXriApKUlrW2dnZ5SWluLSpUsa5VlZWcjNzVXvmNEFGxsbjR0pZZ4chQEAAwMD9OzZE8uWLcPZs2exaNEiJCQk4Ndff62w77I4L1y4UK7u/PnzqFOnDszMzJ7vAp5i6NChOHXqFP7+++8KFwKX2bp1K3r06IGvvvoK/v7+6NWrF7y8vMp9JpVNDivj/v37GDlyJNzd3TF27FhERUUhOTlZZ/0T0fNjQkJ668MPP4SZmRlGjx6NrKyscvVXrlzBypUrATyecgBQbifMsmXLAAA+Pj46i6tJkybIy8tDWlqauuzWrVvYvn27RrucnJxy55bdIOzJrchl6tWrh9atW2PDhg0aP+D/+OMP7N+/X32dYujRowc++ugjrF69Go6Ojk9tV6tWrXKjL1u2bMFff/2lUVaWOFWUvFXV9OnTkZmZiQ0bNmDZsmVo1KgRAgMDn/o5ElH1443RSG81adIEcXFxGDJkCNzc3DTu1Hr06FFs2bIFI0aMAAC0atUKgYGB+Pzzz5Gbm4tu3brhxIkT2LBhAwYOHPjULaXPwt/fH9OnT8egQYMwadIkPHjwAOvWrUOzZs00FnUuWLAAiYmJ8PHxgbOzM7Kzs7F27Vo0aNAAnTt3fmr/S5cuRZ8+feDp6YmgoCA8fPgQq1atgpWVFebNm6ez63iSgYEBZs+erbWdr68vFixYgJEjR6Jjx45IT0/Hpk2b0LhxY412TZo0gbW1NWJiYmBhYQEzMzO0b98eLi4uVYorISEBa9euxdy5c9XbkNevX4/u3bsjIiICUVFRVeqPiEQi8S4fItFdvHhRGDNmjNCoUSPB2NhYsLCwEDp16iSsWrVKKCwsVLcrLi4W5s+fL7i4uAhGRkZCw4YNhfDwcI02gvB426+Pj0+593lyu+nTtv0KgiDs379faNmypWBsbCy4uroK33zzTbltvwcOHBAGDBggKBQKwdjYWFAoFMK7774rXLx4sdx7PLk19pdffhE6deokmJqaCpaWlkK/fv2Es2fParQpe78ntxWvX79eACBkZGQ89TMVBM1tv0/ztG2/U6dOFerVqyeYmpoKnTp1EpKSkircrvvjjz8K7u7ugqGhocZ1duvWTWjRokWF7/nPfvLz8wVnZ2ehTZs2QnFxsUa7KVOmCAYGBkJSUtK/XgMRVQ+ZIFRh5RoRERGRCLiGhIiIiCTHhISIiIgkx4SEiIiIJMeEhIiIiCTHhISIiIgkx4SEiIiIJMeEhIiIiCSnl3dqNX0tROoQiGqke8mrpQ6BqMYxqYafhLr6ufTwlP5+hzlCQkRERJJjQkJERCQ2mYFujipKTExEv379oFAoIJPJsGPHDo36goIChISEoEGDBjA1NYW7uztiYmI02hQWFiI4OBh2dnYwNzeHn59fuQeWZmZmwsfHB7Vr14a9vT3CwsLw6NGjKsXKhISIiEhsMplujiq6f/8+WrVqhTVr1lRYHxoain379uGbb77BuXPnMHnyZISEhGDnzp3qNlOmTMGuXbuwZcsWHDp0CDdv3sTgwYPV9SUlJfDx8VE/uHTDhg2IjY3FnDlzqvYR6eOzbLiGhKhiXENCVF61rCFpN0Un/Tz8ffkznyuTybB9+3YMHDhQXdayZUsMGTIEERER6rK2bduiT58+WLhwIfLy8lC3bl3ExcXh7bffBgCcP38ebm5uSEpKQocOHbB37174+vri5s2bcHBwAADExMRg+vTpuH37NoyNjSsVH0dIiIiIXhAqlQr5+fkah0qleub+OnbsiJ07d+Kvv/6CIAj49ddfcfHiRfTq1QsAkJKSguLiYnh5eanPad68OZycnJCUlAQASEpKgoeHhzoZAQBvb2/k5+fjzJkzlY6FCQkREZHYdDRlExkZCSsrK40jMjLymcNatWoV3N3d0aBBAxgbG6N3795Ys2YNunbtCgBQKpUwNjaGtbW1xnkODg5QKpXqNv9MRsrqy+oqSy+3/RIREdUoz7AgtSLh4eEIDQ3VKJPL5c/c36pVq3Ds2DHs3LkTzs7OSExMRHBwMBQKhcaoSHVgQkJERPSCkMvlz5WA/NPDhw8xc+ZMbN++HT4+PgCAV199Fampqfjkk0/g5eUFR0dHFBUVITc3V2OUJCsrC46OjgAAR0dHnDhxQqPvsl04ZW0qg1M2REREYpNol82/KS4uRnFxMQwMNFOBWrVqobS0FMDjBa5GRkY4cOCAuv7ChQvIzMyEp6cnAMDT0xPp6enIzs5Wt4mPj4elpSXc3d0rHQ9HSIiIiMSmoymbqiooKMDly5fVrzMyMpCamgpbW1s4OTmhW7duCAsLg6mpKZydnXHo0CF8/fXXWLZsGQDAysoKQUFBCA0Nha2tLSwtLTFx4kR4enqiQ4cOAIBevXrB3d0dw4YNQ1RUFJRKJWbPno3g4OAqjeYwISEiItJTv//+O3r06KF+Xbb+JDAwELGxsfjuu+8QHh6OgIAA5OTkwNnZGYsWLcL777+vPmf58uUwMDCAn58fVCoVvL29sXbtWnV9rVq1sHv3bowfPx6enp4wMzNDYGAgFixYUKVYeR8SopcI70NCVF613IfEc4ZO+nmY9LFO+qmJOEJCREQkNommbF4k/ISIiIhIchwhISIiEpuOd8joIyYkREREYuOUjVZMSIiIiMTGERKtmLIRERGR5DhCQkREJDZO2WjFhISIiEhsTEi04idEREREkuMICRERkdgMuKhVGyYkREREYuOUjVb8hIiIiEhyHCEhIiISG+9DohUTEiIiIrFxykYrfkJEREQkOY6QEBERiY1TNloxISEiIhIbp2y0YkJCREQkNo6QaMWUjYiIiCTHERIiIiKxccpGKyYkREREYuOUjVZM2YiIiEhyHCEhIiISG6dstGJCQkREJDZO2WjFlI2IiIgkxxESIiIisXHKRismJERERGJjQqIVPyEiIiKSHEdIiIiIxMZFrVoxISEiIhIbp2y0YkJCREQkNo6QaMWUjYiISE8lJiaiX79+UCgUkMlk2LFjR7k2586dQ//+/WFlZQUzMzO8/vrryMzMVNcXFhYiODgYdnZ2MDc3h5+fH7KysjT6yMzMhI+PD2rXrg17e3uEhYXh0aNHVYqVCQkREZHYZAa6Oaro/v37aNWqFdasWVNh/ZUrV9C5c2c0b94cBw8eRFpaGiIiImBiYqJuM2XKFOzatQtbtmzBoUOHcPPmTQwePFhdX1JSAh8fHxQVFeHo0aPYsGEDYmNjMWfOnKp9RIIgCFW+whrO9LUQqUMgqpHuJa+WOgSiGsekGhYvmA7+Sif9PPwh6JnPlclk2L59OwYOHKgu8/f3h5GRETZu3FjhOXl5eahbty7i4uLw9ttvAwDOnz8PNzc3JCUloUOHDti7dy98fX1x8+ZNODg4AABiYmIwffp03L59G8bGxpWKjyMkRERELwiVSoX8/HyNQ6VSPVNfpaWl2LNnD5o1awZvb2/Y29ujffv2GtM6KSkpKC4uhpeXl7qsefPmcHJyQlJSEgAgKSkJHh4e6mQEALy9vZGfn48zZ85UOh4mJERERCKTyWQ6OSIjI2FlZaVxREZGPlNM2dnZKCgowMcff4zevXtj//79GDRoEAYPHoxDhw4BAJRKJYyNjWFtba1xroODA5RKpbrNP5ORsvqyusriLhsiIiKRyXS0yyY8PByhoaEaZXK5/Jn6Ki0tBQAMGDAAU6ZMAQC0bt0aR48eRUxMDLp16/Z8wVYRR0iIiIheEHK5HJaWlhrHsyYkderUgaGhIdzd3TXK3dzc1LtsHB0dUVRUhNzcXI02WVlZcHR0VLd5ctdN2euyNpXBhISIiEhsMh0dOmRsbIzXX38dFy5c0Ci/ePEinJ2dAQBt27aFkZERDhw4oK6/cOECMjMz4enpCQDw9PREeno6srOz1W3i4+NhaWlZLtn5N5yyISIiEpmupmyqqqCgAJcvX1a/zsjIQGpqKmxtbeHk5ISwsDAMGTIEXbt2RY8ePbBv3z7s2rULBw8eBABYWVkhKCgIoaGhsLW1haWlJSZOnAhPT0906NABANCrVy+4u7tj2LBhiIqKglKpxOzZsxEcHFyl0RsmJERERHrq999/R48ePdSvy9afBAYGIjY2FoMGDUJMTAwiIyMxadIkuLq6Ytu2bejcubP6nOXLl8PAwAB+fn5QqVTw9vbG2rVr1fW1atXC7t27MX78eHh6esLMzAyBgYFYsGBBlWLlfUiIXiK8DwlRedVxHxKLIRt00s/fmwN10k9NxBESIiIikUk1ZfMiYUJCREQkMiYk2nGXDREREUmOIyRERERi4wCJVkxIiIiIRMYpG+04ZUNERESS4wgJERGRyDhCoh0TEiIiIpExIdGOUzZEREQkOY6QEBERiYwjJNoxISEiIhIb8xGtOGVDREREkuMICRERkcg4ZaMdExIiIiKRMSHRjgkJERGRyJiQaCdpQlJUVIQdO3YgKSkJSqUSAODo6IiOHTtiwIABMDY2ljI8IiIiqiaSLWq9fPky3NzcEBgYiFOnTqG0tBSlpaU4deoUhg8fjhYtWuDy5ctShUdERKQ7Mh0dekyyEZLx48fDw8MDp06dgqWlpUZdfn4+hg8fjuDgYPz8888SRUhERKQbnLLRTrKE5MiRIzhx4kS5ZAQALC0t8dFHH6F9+/YSREZERETVTbIpG2tra1y7du2p9deuXYO1tXW1xUNERCQWmUymk0OfSTZCMnr0aAwfPhwRERHo2bMnHBwcAABZWVk4cOAAFi5ciIkTJ0oVHhERkc7oezKhC5IlJAsWLICZmRmWLl2KqVOnqv+yBEGAo6Mjpk+fjg8//FCq8IiIiKgaSbrtd/r06Zg+fToyMjI0tv26uLhIGRYREZFOcYREuxpxYzQXFxcmIUREpL+Yj2jFh+sRERGR5GrECAkREZE+45SNdkxIiIiIRMaERDsmJERERCJjQqKd5GtI9u3bh8OHD6tfr1mzBq1bt8bQoUNx7949CSMjIiKi6iJ5QhIWFob8/HwAQHp6OqZOnYq+ffsiIyMDoaGhEkdHRESkA3y4nlaST9lkZGTA3d0dALBt2zb4+vpi8eLFOHnyJPr27StxdERERM+PUzbaST5CYmxsjAcPHgAAfvnlF/Tq1QsAYGtrqx45ISIioqpLTExEv379oFAoIJPJsGPHjqe2ff/99yGTybBixQqN8pycHAQEBMDS0hLW1tYICgpCQUGBRpu0tDR06dIFJiYmaNiwIaKioqocq+QjJJ07d0ZoaCg6deqEEydOYPPmzQCAixcvokGDBhJHR53aNMGU4V5o4+6EenWt8M6Uz7HrYJq63szUGAsnDUC/Hq/C1soM127exdpvD+HLrf9bF7Rqlj/ebO+KenWtUPBQhWOnMzB75Y+4eC1L3aahow1WzhyCbu2aoeChCpt2HUfEqp0oKSmt1usl0qWsrCysWLYUR377DYWFD9HQyRkLFi5Gi5YeAICImTOw88ftGud07NQZ6z7/SopwSURSjZDcv38frVq1wqhRozB48OCnttu+fTuOHTsGhUJRri4gIAC3bt1CfHw8iouLMXLkSIwdOxZxcXEAgPz8fPTq1QteXl6IiYlBeno6Ro0aBWtra4wdO7bSsUqekKxevRoTJkzA1q1bsW7dOtSvXx8AsHfvXvTu3Vvi6MjMVI70i3/h6x+TsHlZ+f+xlkz1Q/fXm2HkrK9x/eZdeHm6YWX4O7h1Ow97DqUDAE6d+xPf7U3Gn7fuwdaqNma974Pda4PR3HcuSksFGBjI8EP0eGTdzUePEZ/Csa4VvvxoGIoflWDu6l3VfclEOpGfl4cR772Ldm+0x5qYL2Bja4PM69dhaWml0a5T5y5YsDBS/drY2Li6Q6VqIFVC0qdPH/Tp0+df2/z111+YOHEifv75Z/j4+GjUnTt3Dvv27UNycjLatWsHAFi1ahX69u2LTz75BAqFAps2bUJRURH++9//wtjYGC1atEBqaiqWLVv2YiUkTk5O2L17d7ny5cuXSxANPWn/kbPYf+TsU+s7tHLBN7uP47eUSwCA//5wBEF+ndCuhbM6IfnvD0fU7TNv5WD+ml1I/n4mnBV2yLhxB16ebnBr7Aif91chO+dvpF38CwvW7sHCSQOwMOYnFD8qEfciiUTw36++gIOjIz5a9L9ko0GDhuXaGRsbo07dutUZGr3AVCoVVCqVRplcLodcLn+m/kpLSzFs2DCEhYWhRYsW5eqTkpJgbW2tTkYAwMvLCwYGBjh+/DgGDRqEpKQkdO3aVSOZ9vb2xpIlS3Dv3j3Y2NhUKhbJ15CcPHkS6enp6tc//vgjBg4ciJkzZ6KoqEjCyKgyjp3OgG83DyjqPv6tr2u7pmjqbI9fjp2rsH1tE2MM798BGTfu4Iby8bbu9q+64I/LN5Gd87e6XfzRc7CyMIV7k3riXwSRCA79moAWLVpi2pRJ6N7FE+/4DcS2Ld+Xa/d78gl07+KJ/j7eWLhgLnJzebsDfSSTyXRyREZGwsrKSuOIjIzUHsBTLFmyBIaGhpg0aVKF9UqlEvb29hplhoaGsLW1VT8UV6lUwsHBQaNN2euyNpUh+QjJuHHjMGPGDHh4eODq1avw9/fHoEGDsGXLFjx48KDc4hqqWUKXbMGaiHdxZf8iFBeXoFQoxYSPvsWRk1c02o39TxcsmjwQ5rXluJChhM/41eqRDwc7S2Tf/VujfXbO4wXNDnUsgQvVcy1EunTjxp/4fvO3GBY4EkFj38eZ9HQsiVwIIyMj9B84CADQsXMX9PR6C/UbNMCff/6JVSuWYcK4MdgYtxm1atWS+ApIp3Q0YxMeHl7ulhjPOjqSkpKClStX4uTJkzViF5DkCcnFixfRunVrAMCWLVvQtWtXxMXF4ciRI/D399eakFQ0fCWUlkBmwC9zdZjg3w1veDSC3wcxyLyVg85tXsGKGY/XkPx6/H+ZxHd7k3Hg+Hk41rHE5OFe+GbJKLw5chlURY8kjJ5IPKWlAlq0bIlJkx//8HBzc8fly5ew5fvv1AlJn77/m69v2swVzZq5wqe3F35PPoH2HTwliZtqtueZnnnSb7/9huzsbDg5OanLSkpKMHXqVKxYsQLXrl2Do6MjsrOzNc579OgRcnJy4OjoCABwdHREVlaWRpuy12VtKkPyKRtBEFBa+ngnxS+//KK+90jDhg1x584dredXNHz1KCtF1JjpMRO5EeZP7Ifpn/6AnxL/wB+XbiJmcyK27j+JycN6arTNLyjElczbOHLyCoZO+xKuLg4Y8GYrAEDW3XzY21lotLe3tXxcd4dbv+nFVLduXTRu0kSjrHHjxrh16+ZTz2nQsCFsbGyQmXld7PComulqykaXhg0bhrS0NKSmpqoPhUKBsLAw/PzzzwAAT09P5ObmIiXlfz9XExISUFpaivbt26vbJCYmori4WN0mPj4erq6ulV4/AtSAhKRdu3ZYuHAhNm7ciEOHDqlX+GZkZJSbk6pIeHg48vLyNA5Dh7Zih00AjAxrwdjIEKWCoFFeUlIKA4Onf3FkMhlkkMHY6PEA3fG0DLR8RYG6NubqNj07NEfe3w9x7mrl5x+JapLWr7XBtYwMjbLr165Boaj/1HOylErk5uaibh0uctU3UiUkBQUF6mQDePyzNTU1FZmZmbCzs0PLli01DiMjIzg6OsLV1RUA4Obmht69e2PMmDE4ceIEjhw5gpCQEPj7+6u3CA8dOhTGxsYICgrCmTNnsHnzZqxcubLKd1uXfMpmxYoVCAgIwI4dOzBr1iy88sorAICtW7eiY8eOWs+vaPiK0zW6Y2ZqjCYN//ePY6P6dni1WX3cy3+AP5X3kPj7JSyePBAPC4uReSsHXdq+ggDfNzB92Q/q9m97t8WBpHO4c68A9R2sMXVkLzxUFePnw2cAAL8kncO5q0p8tTAQs1bugIOdJeYG++Kz7xNRVMwpHXoxvTc8EIHvvYsvP49BL+8++CM9DVu3fo858xYAAB7cv4+Ydavh9ZY37OrUwY0//8TyT5eioZMzOnbuInH0pGtSLdH4/fff0aNHD/XrsiQhMDAQsbGxlepj06ZNCAkJQc+ePWFgYAA/Pz9ER0er662srLB//34EBwejbdu2qFOnDubMmVOlLb8AIBOEJ369rSEKCwtRq1YtGBkZVflc09dCRIjo5dSlbVPs//KDcuUbdx7D2LnfwMHOAgsmDoCXZ3PYWNZG5q0c/PeHo4j+JgEAUK+uFdbOGYrX3BrCxrI2su/+jcMnL2Px53tx6fr/5iWd6tlg5Ux/dG3bFPcLVdi06wRmR//IG6Pp2L3k1VKH8FI5dPBXRK9Yhszr11C/QQMMGz4Sfv95B8Djf+MmTwzG+fNn8Xf+37C3t4dnx04InvgB7OrUkTjyl4tJNfxq/sq0vTrp5/In/35PkRdZjU1IngcTEqKKMSEhKq86EpKmYft00s+lpfp7w1DJp2xKSkqwfPlyfP/998jMzCx375GcnByJIiMiItKNGrCrtsaTfFHr/PnzsWzZMgwZMgR5eXkIDQ3F4MGDYWBggHnz5kkdHhEREVUDyROSTZs24YsvvsDUqVNhaGiId999F19++SXmzJmDY8eOSR0eERHRc6uJ235rGskTEqVSCQ+Px0++NDc3R15eHgDA19cXe/bskTI0IiIinZDJdHPoM8kTkgYNGuDWrVsAgCZNmmD//v0AgOTkZJ3djY6IiIhqNskTkkGDBuHAgQMAgIkTJyIiIgJNmzbF8OHDMWrUKImjIyIien4GBjKdHPpM8l02H3/8sfrPQ4YMgZOTE5KSktC0aVP069dPwsiIiIh0Q9+nW3RB8oTkSZ6envD05EOliIiIXiaSJCQ7d+6sdNv+/fuLGAkREZH49H2HjC5IkpAMHDiwUu1kMhlKSkrEDYaIiEhkzEe0kyQhKS3l80mIiOjlwRES7STfZUNEREQkWUKSkJAAd3d35Ofnl6vLy8tDixYtkJiYKEFkREREusU7tWonWUKyYsUKjBkzBpaWluXqrKysMG7cOCxfvlyCyIiIiHSLd2rVTrKE5PTp0+jd++mPUe7VqxdSUlKqMSIiIiKSimT3IcnKyoKRkdFT6w0NDXH79u1qjIiIiEgc+j7doguSjZDUr18ff/zxx1Pr09LSUK9evWqMiIiISBycstFOsoSkb9++iIiIQGFhYbm6hw8fYu7cufD19ZUgMiIiIqpukk3ZzJ49Gz/88AOaNWuGkJAQuLq6AgDOnz+PNWvWoKSkBLNmzZIqPCIiIp3hlI12kiUkDg4OOHr0KMaPH4/w8HAIggDg8V+at7c31qxZAwcHB6nCIyIi0hnmI9pJ+nA9Z2dn/PTTT7h37x4uX74MQRDQtGlT2NjYSBkWERERVbMa8bRfGxsbvP7661KHQUREJApO2WhXIxISIiIifcZ8RDsmJERERCLjCIl2fLgeERERSY4jJERERCLjAIl2TEiIiIhExikb7ThlQ0RERJLjCAkREZHIOECiHRMSIiIikXHKRjtO2RAREempxMRE9OvXDwqFAjKZDDt27FDXFRcXY/r06fDw8ICZmRkUCgWGDx+OmzdvavSRk5ODgIAAWFpawtraGkFBQSgoKNBok5aWhi5dusDExAQNGzZEVFRUlWNlQkJERCQymUw3R1Xdv38frVq1wpo1a8rVPXjwACdPnkRERAROnjyJH374ARcuXED//v012gUEBODMmTOIj4/H7t27kZiYiLFjx6rr8/Pz0atXLzg7OyMlJQVLly7FvHnz8Pnnn1ftMxLKnmqnR0xfC5E6BKIa6V7yaqlDIKpxTKph8UKXTw/rpJ/fpnZ+5nNlMhm2b9+OgQMHPrVNcnIy3njjDVy/fh1OTk44d+4c3N3dkZycjHbt2gEA9u3bh759++LGjRtQKBRYt24dZs2aBaVSCWNjYwDAjBkzsGPHDpw/f77S8XGEhIiI6AWhUqmQn5+vcahUKp31n5eXB5lMBmtrawBAUlISrK2t1ckIAHh5ecHAwADHjx9Xt+natas6GQEAb29vXLhwAffu3av0ezMhISIiEplMJtPJERkZCSsrK40jMjJSJzEWFhZi+vTpePfdd2FpaQkAUCqVsLe312hnaGgIW1tbKJVKdRsHBweNNmWvy9pUBnfZEBERiUxXm2zCw8MRGhqqUSaXy5+73+LiYrzzzjsQBAHr1q177v6eBRMSIiIikelq269cLtdJAvJPZcnI9evXkZCQoB4dAQBHR0dkZ2drtH/06BFycnLg6OiobpOVlaXRpux1WZvK4JQNERHRS6osGbl06RJ++eUX2NnZadR7enoiNzcXKSkp6rKEhASUlpaiffv26jaJiYkoLi5Wt4mPj4erqytsbGwqHQsTEiIiIpFJte23oKAAqampSE1NBQBkZGQgNTUVmZmZKC4uxttvv43ff/8dmzZtQklJCZRKJZRKJYqKigAAbm5u6N27N8aMGYMTJ07gyJEjCAkJgb+/PxQKBQBg6NChMDY2RlBQEM6cOYPNmzdj5cqV5aaWtH5G3PZL9PLgtl+i8qpj2++b0Uk66SdhkmeV2h88eBA9evQoVx4YGIh58+bBxcWlwvN+/fVXdO/eHcDjG6OFhIRg165dMDAwgJ+fH6Kjo2Fubq5un5aWhuDgYCQnJ6NOnTqYOHEipk+fXqVYmZAQvUSYkBCVp88JyYuEi1qJiIhExkfZaMeEhIiISGQGzEi04qJWIiIikhxHSIiIiETGARLtmJAQERGJTFc3RtNnTEiIiIhEZsB8RCuuISEiIiLJcYSEiIhIZJyy0Y4JCRERkciYj2jHKRsiIiKSHEdIiIiIRCYDh0i0YUJCREQkMu6y0Y5TNkRERCQ5jpAQERGJjLtstGNCQkREJDLmI9pxyoaIiIgkxxESIiIikRlwiEQrJiREREQiYz6iHRMSIiIikXFRq3ZcQ0JERESS4wgJERGRyDhAoh0TEiIiIpFxUat2nLIhIiIiyXGEhIiISGQcH9GOCQkREZHIuMtGO07ZEBERkeQ4QkJERCQyAw6QaFWphGTnzp2V7rB///7PHAwREZE+4pSNdpVKSAYOHFipzmQyGUpKSp4nHiIiInoJVSohKS0tFTsOIiIivcUBEu24hoSIiEhknLLR7pl22dy/fx8//fQTYmJiEB0drXEQERGRJgOZbo6qSkxMRL9+/aBQKCCTybBjxw6NekEQMGfOHNSrVw+mpqbw8vLCpUuXNNrk5OQgICAAlpaWsLa2RlBQEAoKCjTapKWloUuXLjAxMUHDhg0RFRVV5VirPEJy6tQp9O3bFw8ePMD9+/dha2uLO3fuoHbt2rC3t8ekSZOqHAQRERHp3v3799GqVSuMGjUKgwcPLlcfFRWF6OhobNiwAS4uLoiIiIC3tzfOnj0LExMTAEBAQABu3bqF+Ph4FBcXY+TIkRg7dizi4uIAAPn5+ejVqxe8vLwQExOD9PR0jBo1CtbW1hg7dmylY5UJgiBU5eK6d++OZs2aISYmBlZWVjh9+jSMjIzw3nvv4YMPPqjwgqub6WshUodAVCPdS14tdQhENY5JNSxeGPlduk76We/v8cznymQybN++Xb1RRRAEKBQKTJ06FdOmTQMA5OXlwcHBAbGxsfD398e5c+fg7u6O5ORktGvXDgCwb98+9O3bFzdu3IBCocC6deswa9YsKJVKGBsbAwBmzJiBHTt24Pz585WOr8pTNqmpqZg6dSoMDAxQq1YtqFQq9fDMzJkzq9odERGR3pPp6FCpVMjPz9c4VCrVM8WUkZEBpVIJLy8vdZmVlRXat2+PpKQkAEBSUhKsra3VyQgAeHl5wcDAAMePH1e36dq1qzoZAQBvb29cuHAB9+7dq3Q8VU5IjIyMYGDw+DR7e3tkZmaqL+LPP/+sandERERUSZGRkbCystI4IiMjn6kvpVIJAHBwcNAod3BwUNcplUrY29tr1BsaGsLW1lajTUV9/PM9KqPKA1WvvfYakpOT0bRpU3Tr1g1z5szBnTt3sHHjRrRs2bKq3REREek9Ax3tsgkPD0doaKhGmVwu10nfUqvyCMnixYtRr149AMCiRYtgY2OD8ePH4/bt2/j88891HiAREdGLTibTzSGXy2FpaalxPGtC4ujoCADIysrSKM/KylLXOTo6Ijs7W6P+0aNHyMnJ0WhTUR//fI/KqHJC0q5dO/To0QPA4ymbffv2IT8/HykpKWjVqlVVuyMiIiIJuLi4wNHREQcOHFCX5efn4/jx4/D09AQAeHp6Ijc3FykpKeo2CQkJKC0tRfv27dVtEhMTUVxcrG4THx8PV1dX2NjYVDoePu2XiIhIZDKZTCdHVRUUFCA1NRWpqakAHi9kTU1NRWZmJmQyGSZPnoyFCxdi586dSE9Px/Dhw6FQKNQ7cdzc3NC7d2+MGTMGJ06cwJEjRxASEgJ/f38oFAoAwNChQ2FsbIygoCCcOXMGmzdvxsqVK8tNLWlT5TUkLi4u//qhXL16tapdEhER6TWpbtT6+++/q2c1AKiThMDAQMTGxuLDDz/E/fv3MXbsWOTm5qJz587Yt2+f+h4kALBp0yaEhISgZ8+eMDAwgJ+fn8aNUK2srLB//34EBwejbdu2qFOnDubMmVOle5AAz3AfkpUrV2q8Li4uxqlTp7Bv3z6EhYVhxowZVQpADLwPCVHFeB8SovKq4z4k47ae0Uk/n73dQif91ERV/mv44IMPKixfs2YNfv/99+cOiIiISN/oapeNPtPZGpI+ffpg27ZtuuqOiIhIb+hql40+09lA1datW2Fra6ur7oiIiPQGn/ar3TPdGO2fH6wgCFAqlbh9+zbWrl2r0+CIiIjo5VDlhGTAgAEaCYmBgQHq1q2L7t27o3nz5joN7lndPb5K6hCIaqTiklKpQyCqcUwMxb8DBu+xoV2VE5J58+aJEAYREZH+4pSNdlVO2mrVqlXuNrIAcPfuXdSqVUsnQREREdHLpcojJE+7bYlKpdJ49DARERE9ZsABEq0qnZCU3ZVNJpPhyy+/hLm5ubqupKQEiYmJNWYNCRERUU3ChES7Sicky5cvB/B4hCQmJkZjesbY2BiNGjVCTEyM7iMkIiIivVfphCQjIwMA0KNHD/zwww9VeoIfERHRy4yLWrWr8hqSX3/9VYw4iIiI9BanbLSr8i4bPz8/LFmypFx5VFQU/vOf/+gkKCIiInq5VDkhSUxMRN++fcuV9+nTB4mJiToJioiISJ/wWTbaVXnKpqCgoMLtvUZGRsjPz9dJUERERPqET/vVrsojJB4eHti8eXO58u+++w7u7u46CYqIiEifGOjo0GdVHiGJiIjA4MGDceXKFbz55psAgAMHDiAuLg5bt27VeYBERESk/6qckPTr1w87duzA4sWLsXXrVpiamqJVq1ZISEiAra2tGDESERG90Dhjo12VExIA8PHxgY+PDwAgPz8f3377LaZNm4aUlBSUlJToNEAiIqIXHdeQaPfMU1KJiYkIDAyEQqHAp59+ijfffBPHjh3TZWxERET0kqjSCIlSqURsbCy++uor5Ofn45133oFKpcKOHTu4oJWIiOgpOECiXaVHSPr16wdXV1ekpaVhxYoVuHnzJlatWiVmbERERHrBQKabQ59VeoRk7969mDRpEsaPH4+mTZuKGRMRERG9ZCo9QnL48GH8/fffaNu2Ldq3b4/Vq1fjzp07YsZGRESkFwxkMp0c+qzSCUmHDh3wxRdf4NatWxg3bhy+++47KBQKlJaWIj4+Hn///beYcRIREb2weOt47aq8y8bMzAyjRo3C4cOHkZ6ejqlTp+Ljjz+Gvb09+vfvL0aMREREpOee6060rq6uiIqKwo0bN/Dtt9/qKiYiIiK9wkWt2j3TjdGeVKtWLQwcOBADBw7URXdERER6RQY9zyZ0QCcJCRERET2dvo9u6IK+PzyQiIiIXgAcISEiIhIZR0i04wgJERGRyGQymU6OqigpKUFERARcXFxgamqKJk2a4KOPPoIgCOo2giBgzpw5qFevHkxNTeHl5YVLly5p9JOTk4OAgABYWlrC2toaQUFBKCgo0Mnn8k9MSIiIiPTQkiVLsG7dOqxevRrnzp3DkiVLEBUVpfHYl6ioKERHRyMmJgbHjx+HmZkZvL29UVhYqG4TEBCAM2fOID4+Hrt370ZiYiLGjh2r83hlwj9TJT3xoEjvLolIJ0r07+tO9Nws5OL/bv7poas66Wdqt8aVbuvr6wsHBwd89dVX6jI/Pz+Ymprim2++gSAIUCgUmDp1KqZNmwYAyMvLg4ODA2JjY+Hv749z587B3d0dycnJaNeuHQBg37596Nu3L27cuAGFQqGT6wI4QkJERCQ6Xd2pVaVSIT8/X+NQqVQVvmfHjh1x4MABXLx4EQBw+vRpHD58GH369AEAZGRkQKlUwsvLS32OlZUV2rdvj6SkJABAUlISrK2t1ckIAHh5ecHAwADHjx/X6WfEhISIiOgFERkZCSsrK40jMjKywrYzZsyAv78/mjdvDiMjI7z22muYPHkyAgICAABKpRIA4ODgoHGeg4ODuk6pVMLe3l6j3tDQELa2tuo2usJdNkRERCLT1YPxwsPDERoaqlEml8srbPv9999j06ZNiIuLQ4sWLZCamorJkydDoVAgMDBQJ/HoEhMSIiIikelq269cLn9qAvKksLAw9SgJAHh4eOD69euIjIxEYGAgHB0dAQBZWVmoV6+e+rysrCy0bt0aAODo6Ijs7GyNfh89eoScnBz1+brCKRsiIiI99ODBAxgYaP6Yr1WrFkpLSwEALi4ucHR0xIEDB9T1+fn5OH78ODw9PQEAnp6eyM3NRUpKirpNQkICSktL0b59e53GyxESIiIikeloxqZK+vXrh0WLFsHJyQktWrTAqVOnsGzZMowaNer/Y5Jh8uTJWLhwIZo2bQoXFxdERERAoVCon03n5uaG3r17Y8yYMYiJiUFxcTFCQkLg7++v0x02ABMSIiIi0RlI8HC9VatWISIiAhMmTEB2djYUCgXGjRuHOXPmqNt8+OGHuH//PsaOHYvc3Fx07twZ+/btg4mJibrNpk2bEBISgp49e8LAwAB+fn6Ijo7Weby8DwnRS4T3ISEqrzruQ7L26DWd9DOhYyOd9FMTcQ0JERERSY5TNkRERCLjw/W0Y0JCREQkMl3dh0SfccqGiIiIJMcREiIiIpFxgEQ7JiREREQi45SNdpyyISIiIslxhISIiEhkHCDRjgkJERGRyDgdoR0/IyIiIpIcR0iIiIhEJuOcjVZMSIiIiETGdEQ7JiREREQi47Zf7biGhIiIiCTHERIiIiKRcXxEOyYkREREIuOMjXacsiEiIiLJcYSEiIhIZNz2qx0TEiIiIpFxOkI7fkZEREQkOY6QEBERiYxTNtoxISEiIhIZ0xHtOGVDREREkuMICRERkcg4ZaMdExIiIiKRcTpCOyYkREREIuMIiXZM2oiIiEhyHCEhIiISGcdHtGNCQkREJDLO2GjHKRsiIiKSHBMSIiIikRlAppOjqv766y+89957sLOzg6mpKTw8PPD777+r6wVBwJw5c1CvXj2YmprCy8sLly5d0ugjJycHAQEBsLS0hLW1NYKCglBQUPDcn8mTamxCkpWVhQULFkgdBhER0XOTyXRzVMW9e/fQqVMnGBkZYe/evTh79iw+/fRT2NjYqNtERUUhOjoaMTExOH78OMzMzODt7Y3CwkJ1m4CAAJw5cwbx8fHYvXs3EhMTMXbsWF19NGoyQRAEnfeqA6dPn0abNm1QUlJS5XMfFNXISyKSXEnN/LoTScpCLv7v5rv/yNJJP74tHSrddsaMGThy5Ah+++23CusFQYBCocDUqVMxbdo0AEBeXh4cHBwQGxsLf39/nDt3Du7u7khOTka7du0AAPv27UPfvn1x48YNKBSK57+o/yfZota0tLR/rb9w4UI1RUJERCQumQT7bHbu3Alvb2/85z//waFDh1C/fn1MmDABY8aMAQBkZGRAqVTCy8tLfY6VlRXat2+PpKQk+Pv7IykpCdbW1upkBAC8vLxgYGCA48ePY9CgQTqLV7KEpHXr1pDJZKhogKasnDeSISIifaCrH2cqlQoqlUqjTC6XQy6Xl2t79epVrFu3DqGhoZg5cyaSk5MxadIkGBsbIzAwEEqlEgDg4KA56uLg4KCuUyqVsLe316g3NDSEra2tuo2uSLaGxNbWFl988QUyMjLKHVevXsXu3bulCo2IiKhGioyMhJWVlcYRGRlZYdvS0lK0adMGixcvxmuvvYaxY8dizJgxiImJqeaoK0eyEZK2bdvi5s2bcHZ2rrA+Nze3wtETIiKiF82z7JCpSHh4OEJDQzXKKhodAYB69erB3d1do8zNzQ3btm0DADg6OgJ4vImkXr166jZZWVlo3bq1uk12drZGH48ePUJOTo76fF2RbITk/fffR6NGjZ5a7+TkhPXr11dfQERERCLR1S4buVwOS0tLjeNpCUmnTp3Krce8ePGieiDAxcUFjo6OOHDggLo+Pz8fx48fh6enJwDA09MTubm5SElJUbdJSEhAaWkp2rdvr9vPqKbusnke3GVDVDHusiEqrzp22ew/d1sn/fRyq1vptsnJyejYsSPmz5+Pd955BydOnMCYMWPw+eefIyAgAACwZMkSfPzxx9iwYQNcXFwQERGBtLQ0nD17FiYmJgCAPn36ICsrCzExMSguLsbIkSPRrl07xMXF6eSayjAhIXqJMCEhKk9fExIA2L17N8LDw3Hp0iW4uLggNDRUvcsGeLz1d+7cufj888+Rm5uLzp07Y+3atWjWrJm6TU5ODkJCQrBr1y4YGBjAz88P0dHRMDc318k1lWFCQvQSYUJCVF51JCTx5+7opJ+33OropJ+aiA/XIyIiEpkB72KhVY29dTwRERG9PDhCQkREJDIp7tT6opF8hGTfvn04fPiw+vWaNWvQunVrDB06FPfu3ZMwMiIiIt2Q4uF6LxrJE5KwsDDk5+cDANLT0zF16lT07dsXGRkZ5W7+QkRERPpJ8imbjIwM9Z3ktm3bBl9fXyxevBgnT55E3759JY6OiIjo+XHKRjvJR0iMjY3x4MEDAMAvv/yCXr16AXj8rJuykRMiIqIXmYFMN4c+k3yEpHPnzggNDUWnTp1w4sQJbN68GcDj29s2aNBA4uiIiIioOkg+QrJ69WoYGhpi69atWLduHerXrw8A2Lt3L3r37i1xdFRZ9+8XYOmSxejT6010aNcKge/548wf6ep6QRCwdnU03urRBR3atcK40SNx/fo16QIm0rGTvydjSsh49O7ZFe1edcPBhF806gVBQMyaaHi/2QWdXm+NCWNGIvOJ78D5s2cwYewodO/0Bnp26YBF8+fgwYP71XgVJBaZjv7TZ5InJE5OTti9ezdOnz6NoKAgdfny5csRHR0tYWRUFQvmRuBY0lEsXLwE3/+wE54dO+H9MSORnZUFAIj975f4Nm4jZkbMw9ebvoepqSmCx42GSqWSOHIi3Xj48CGaurpi+syICus3rP8S38V9g/CIeYjdtBkmprUx8f0x6u/A7exsTBgbhIYNnRD7zWZEr/sCV65cxrzZM6vzMkgk3GWjneQJycmTJ5Ge/r/fpH/88UcMHDgQM2fORFFRkYSRUWUVFhbiwC/7MTl0Gtq2ex1OTs54f8JENGzohC2bv4UgCIj75muMGfs+erzZE81cXfHR4iW4fTsbvz7xWyTRi6pTl66YMHEyevR8q1ydIAj49puvETTmfXTv0RNNm7liwaKPcft2tnok5bfEgzA0NMT0WXPQyMUFLVp6YObseUj4ZT/+zLxe3ZdDOibT0aHPJE9Ixo0bh4sXLwIArl69Cn9/f9SuXRtbtmzBhx9+KHF0VBklJY9QUlICY2PNR2DLTUxw6lQK/rpxA3fu3Eb7Dh3VdRYWFmjp8SrSTqdWc7RE1e+vv27g7p07eKODp7rM/P+/A+mnTwMAioqKYGRkBAOD//2zLDd5/J1KPXWyegMmkoDkCcnFixfRunVrAMCWLVvQtWtXxMXFITY2Ftu2bdN6vkqlQn5+vsbBaYDqZWZmjldbtcYXn61FdnYWSkpKsGfXTqSdTsWdO7dx5+7jp1za2tlpnGdnVwd37+jmgVNENVnZ/+d2T3wHbO3q4O7/fz9ef6M97ty9g6/Xf4Xi4iLk5+dh1YplAIA7t3XzpFiSjoFMppNDn0mekAiCgNLSUgCPt/2W3XukYcOGuFOJH1aRkZGwsrLSOD6JihQ1ZipvYWQUBEGAd89uaN/2VXwbtxG9+/jAQCb5/2JEL4QmrzTF/I8isenrWHR+ow28e3RB/foNYGdXBzJ93+/5EuCUjXaSb/tt164dFi5cCC8vLxw6dAjr1q0D8PiGaQ4ODlrPDw8PL3dH1xKZsSix0tM1bOiEr2K/wcMHD1BwvwB169pj+rQpqN+gIerY1QUA5Ny9i7p17dXn3L17B67N3aQKmaja2NV5/Mj4u3fvos4/vgM5d++gmev/vgO9fXzR28cXd+/egampKWSQYdPGWDRo0LDaYyaqbpL/+rpixQqcPHkSISEhmDVrFl555RUAwNatW9GxY0ctZwNyuRyWlpYah1wu13oeicO0dm3UrWuP/Lw8HD16GN17vIn6DRqgTp26OH48Sd2uoKAAf6Sn4dVWraULlqia1K/fAHZ16iD5+DF1Wdl3wKNVq3Lt7ezqoHZtM+z/eS+MjeUa66/oBcUhEq0kHyF59dVXNXbZlFm6dClq1aolQUT0LI4e+Q2CADRq5II/M69j+bKlcHFpjP4DB0Mmk2Hoe8Px5WcxcHJqhPr162Pt6mjUrWuPHm96SR06kU48eHAff2Zmql//9dcNXDh/DlZWVnCsp8C77w3HV5/HoKGTM+rXb4B1ax5/B7r/4zuw+dtNaNWqNUxr18bxY0exctknmPhBKCwsLaW4JNIhfb+HiC5InpA8jYmJidQhUBUU/F2AVSuXIStLCSsra/T0egvBk6bAyMgIADBi1Gg8fPgQC+fPwd9/56P1a22xJuYLjmaR3jh75gzeDwpUv16+dAkAwLf/QMxbGInAkaNR+PAhFi+Y+//fgTaIXve5xnfgTHoaPl+7Cg8ePEAjl8aYGTEPPv0GVPu1EElBJgiCIGUAJSUlWL58Ob7//ntkZmaWu/dITk5Olft8UCTpJRHVWCXSft2JaiQLufirF05czdNJP280ttJJPzWR5GtI5s+fj2XLlmHIkCHIy8tDaGgoBg8eDAMDA8ybN0/q8IiIiJ4bl5BoJ/kISZMmTRAdHQ0fHx9YWFggNTVVXXbs2DHExcVVuU+OkBBVjCMkROVVxwhJso5GSF7nCIl4lEolPDw8AADm5ubIy3v8l+br64s9e/ZIGRoREZFucIhEK8kTkgYNGuDWrVsAHo+W7N+/HwCQnJzMBY9ERKQX+LRf7SRPSAYNGoQDBw4AACZOnIiIiAg0bdoUw4cPx6hRoySOjoiI6Pnxab/aSb6G5ElJSUlISkpC06ZN0a9fv2fqg2tIiCrGNSRE5VXHGpKUa/k66adtI/29J02NS0h0gQkJUcWYkBCVVx0JyUkdJSRt9DghkeTGaDt37qx02/79+4sYCRERUTXQ8+kWXZAkIRk4cGCl2slkMpSUlIgbDBEREUlOkoSktLRUirclIiKShL7vkNGFGvssGyIiIn2h7ztkdEGybb8JCQlwd3dHfn75hT55eXlo0aIFEhMTJYiMiIiIqptkCcmKFSswZswYWFbwWG0rKyuMGzcOy5cvlyAyIiIi3aoJN2r9+OOPIZPJMHnyZHVZYWEhgoODYWdnB3Nzc/j5+SErK0vjvMzMTPj4+KB27dqwt7dHWFgYHj169JzRlCdZQnL69Gn07t37qfW9evVCSkpKNUZEREQkEokzkuTkZHz22Wd49dVXNcqnTJmCXbt2YcuWLTh06BBu3ryJwYMHq+tLSkrg4+ODoqIiHD16FBs2bEBsbCzmzJnz7ME8hWQJSVZWFoyMjJ5ab2hoiNu3b1djRERERPqnoKAAAQEB+OKLL2BjY6Muz8vLw1dffYVly5bhzTffRNu2bbF+/XocPXoUx44dAwDs378fZ8+exTfffIPWrVujT58++Oijj7BmzRoUFRXpNE7JEpL69evjjz/+eGp9Wloa6tWrV40RERERiUPKZ9kEBwfDx8cHXl5eGuUpKSkoLi7WKG/evDmcnJyQlJQE4PHd0z08PODg4KBu4+3tjfz8fJw5c+aZ4nkayXbZ9O3bFxEREejduzdMTEw06h4+fIi5c+fC19dXouiIiIh0R1e7bFQqFVQqlUaZXC5/6sNov/vuO5w8eRLJycnl6pRKJYyNjWFtba1R7uDgAKVSqW7zz2SkrL6sTpckGyGZPXs2cnJy0KxZM0RFReHHH3/Ejz/+iCVLlsDV1RU5OTmYNWuWVOERERHpjK6WkERGRsLKykrjiIyMrPA9//zzT3zwwQfYtGlTuV/8ayLJRkgcHBxw9OhRjB8/HuHh4Sh7pI5MJoO3tzfWrFlTLisjIiJ6mYWHhyM0NFSj7GmjIykpKcjOzkabNm3UZSUlJUhMTMTq1avx888/o6ioCLm5uRqjJFlZWXB0dAQAODo64sSJExr9lu3CKWujK5LeGM3Z2Rk//fQT7t27h8uXL0MQBDRt2lRj0Q0REdELT0dTNv82PfOknj17Ij09XaNs5MiRaN68OaZPn46GDRvCyMgIBw4cgJ+fHwDgwoULyMzMhKenJwDA09MTixYtQnZ2Nuzt7QEA8fHxsLS0hLu7u24u6v/ViDu12tjY4PXXX5c6DCIiIlFIcet4CwsLtGzZUqPMzMwMdnZ26vKgoCCEhobC1tYWlpaWmDhxIjw9PdGhQwcAj2/B4e7ujmHDhiEqKgpKpRKzZ89GcHBwpROjyqoRCQkRERFVv+XLl8PAwAB+fn5QqVTw9vbG2rVr1fW1atXC7t27MX78eHh6esLMzAyBgYFYsGCBzmORCWWLN/TIgyK9uyQinSjRv6870XOzkIu/v+Pszfs66cddYaaTfmoijpAQERGJjM/W006ybb9EREREZThCQkREJDYOkWjFhISIiEhkUuyyedFwyoaIiIgkxxESIiIikenqWTb6jAkJERGRyJiPaMeEhIiISGzMSLTiGhIiIiKSHEdIiIiIRMZdNtoxISEiIhIZF7VqxykbIiIikhxHSIiIiETGARLtmJAQERGJjRmJVpyyISIiIslxhISIiEhk3GWjHRMSIiIikXGXjXacsiEiIiLJcYSEiIhIZBwg0Y4JCRERkdiYkWjFhISIiEhkXNSqHdeQEBERkeQ4QkJERCQy7rLRjgkJERGRyJiPaMcpGyIiIpIcR0iIiIhExikb7ZiQEBERiY4ZiTacsiEiIiLJcYSEiIhIZJyy0Y4JCRERkciYj2jHKRsiIiKSHBMSIiIikclkujmqIjIyEq+//josLCxgb2+PgQMH4sKFCxptCgsLERwcDDs7O5ibm8PPzw9ZWVkabTIzM+Hj44PatWvD3t4eYWFhePTo0fN+JOUwISEiIhKZTEf/VcWhQ4cQHByMY8eOIT4+HsXFxejVqxfu37+vbjNlyhTs2rULW7ZswaFDh3Dz5k0MHjxYXV9SUgIfHx8UFRXh6NGj2LBhA2JjYzFnzhydfTZlZIIgCDrvVWIPivTukoh0okT/vu5Ez81CLv7v5sr8Yp3042hp9Mzn3r59G/b29jh06BC6du2KvLw81K1bF3FxcXj77bcBAOfPn4ebmxuSkpLQoUMH7N27F76+vrh58yYcHBwAADExMZg+fTpu374NY2NjnVwXwBESIiKiF4ZKpUJ+fr7GoVKpKnVuXl4eAMDW1hYAkJKSguLiYnh5eanbNG/eHE5OTkhKSgIAJCUlwcPDQ52MAIC3tzfy8/Nx5swZXV0WACYkREREopPp6IiMjISVlZXGERkZqfX9S0tLMXnyZHTq1AktW7YEACiVShgbG8Pa2lqjrYODA5RKpbrNP5ORsvqyOl3itl8iIiKR6eo+JOHh4QgNDdUok8vlWs8LDg7GH3/8gcOHD+smEBEwISEiInpByOXySiUg/xQSEoLdu3cjMTERDRo0UJc7OjqiqKgIubm5GqMkWVlZcHR0VLc5ceKERn9lu3DK2ugKp2yIiIhEJsUuG0EQEBISgu3btyMhIQEuLi4a9W3btoWRkREOHDigLrtw4QIyMzPh6ekJAPD09ER6ejqys7PVbeLj42FpaQl3d/fn+ETK4y4bopcId9kQlVcdu2xuF+jmvh11zSs/sTFhwgTExcXhxx9/hKurq7rcysoKpqamAIDx48fjp59+QmxsLCwtLTFx4kQAwNGjRwE83vbbunVrKBQKREVFQalUYtiwYRg9ejQWL16sk2sqw4SE6CXChISoPH1NSGRPWbiyfv16jBgxAsDjG6NNnToV3377LVQqFby9vbF27VqN6Zjr169j/PjxOHjwIMzMzBAYGIiPP/4Yhoa6XfXBhIToJcKEhKi86khI7ugoIalThYTkRaO/V0ZERFRD8Gm/2nFRKxEREUmOIyREREQiq+oOmZcRExIiIiKRccpGO07ZEBERkeSYkBAREZHkOGVDREQkMk7ZaMeEhIiISGRc1Kodp2yIiIhIchwhISIiEhmnbLRjQkJERCQy5iPaccqGiIiIJMcREiIiIrFxiEQrJiREREQi4y4b7ThlQ0RERJLjCAkREZHIuMtGOyYkREREImM+oh0TEiIiIrExI9GKa0iIiIhIchwhISIiEhl32WjHhISIiEhkXNSqHadsiIiISHIyQRAEqYMg/aRSqRAZGYnw8HDI5XKpwyGqMfjdICqPCQmJJj8/H1ZWVsjLy4OlpaXU4RDVGPxuEJXHKRsiIiKSHBMSIiIikhwTEiIiIpIcExISjVwux9y5c7loj+gJ/G4QlcdFrURERCQ5jpAQERGR5JiQEBERkeSYkBAREZHkmJBQpclkMuzYsUPqMIhqFH4viHSDCQkBAJRKJSZOnIjGjRtDLpejYcOG6NevHw4cOCB1aAAAQRAwZ84c1KtXD6ampvDy8sKlS5ekDov0XE3/Xvzwww/o1asX7OzsIJPJkJqaKnVIRM+MCQnh2rVraNu2LRISErB06VKkp6dj37596NGjB4KDg6UODwAQFRWF6OhoxMTE4Pjx4zAzM4O3tzcKCwulDo301Ivwvbh//z46d+6MJUuWSB0K0fMT6KXXp08foX79+kJBQUG5unv37qn/DEDYvn27+vWHH34oNG3aVDA1NRVcXFyE2bNnC0VFRer61NRUoXv37oK5ublgYWEhtGnTRkhOThYEQRCuXbsm+Pr6CtbW1kLt2rUFd3d3Yc+ePRXGV1paKjg6OgpLly5Vl+Xm5gpyuVz49ttvn/PqiSpW078X/5SRkSEAEE6dOvXM10skNUOJ8yGSWE5ODvbt24dFixbBzMysXL21tfVTz7WwsEBsbCwUCgXS09MxZswYWFhY4MMPPwQABAQE4LXXXsO6detQq1YtpKamwsjICAAQHByMoqIiJCYmwszMDGfPnoW5uXmF75ORkQGlUgkvLy91mZWVFdq3b4+kpCT4+/s/xydAVN6L8L0g0jdMSF5yly9fhiAIaN68eZXPnT17tvrPjRo1wrRp0/Ddd9+p/+HNzMxEWFiYuu+mTZuq22dmZsLPzw8eHh4AgMaNGz/1fZRKJQDAwcFBo9zBwUFdR6RLL8L3gkjfcA3JS054jhv1bt68GZ06dYKjoyPMzc0xe/ZsZGZmqutDQ0MxevRoeHl54eOPP8aVK1fUdZMmTcLChQvRqVMnzJ07F2lpac91HUS6xO8FUfVjQvKSa9q0KWQyGc6fP1+l85KSkhAQEIC+ffti9+7dOHXqFGbNmoWioiJ1m3nz5uHMmTPw8fFBQkIC3N3dsX37dgDA6NGjcfXqVQwbNgzp6elo164dVq1aVeF7OTo6AgCysrI0yrOystR1RLr0InwviPSOtEtYqCbo3bt3lRfvffLJJ0Ljxo012gYFBQlWVlZPfR9/f3+hX79+FdbNmDFD8PDwqLCubFHrJ598oi7Ly8vjolYSVU3/XvwTF7WSPuAICWHNmjUoKSnBG2+8gW3btuHSpUs4d+4coqOj4enpWeE5TZs2RWZmJr777jtcuXIF0dHR6t/yAODhw4cICQnBwYMHcf36dRw5cgTJyclwc3MDAEyePBk///wzMjIycPLkSfz666/quifJZDJMnjwZCxcuxM6dO5Geno7hw4dDoVBg4MCBOv88iICa/70AHi++TU1NxdmzZwEAFy5cQGpqKtdW0YtJ6oyIaoabN28KwcHBgrOzs2BsbCzUr19f6N+/v/Drr7+q2+CJ7Y1hYWGCnZ2dYG5uLgwZMkRYvny5+jdBlUol+Pv7Cw0bNhSMjY0FhUIhhISECA8fPhQEQRBCQkKEJk2aCHK5XKhbt64wbNgw4c6dO0+Nr7S0VIiIiBAcHBwEuVwu9OzZU7hw4YIYHwWRWk3/Xqxfv14AUO6YO3euCJ8GkbhkgvAcq7eIiIiIdIBTNkRERCQ5JiREREQkOSYkREREJDkmJERERCQ5JiREREQkOSYkREREJDkmJERERCQ5JiREemjEiBEad7Ht3r07Jk+eXO1xHDx4EDKZDLm5udX+3kT0YmFCQlSNRowYAZlMBplMBmNjY7zyyitYsGABHj16JOr7/vDDD/joo48q1ZZJBBFJwVDqAIheNr1798b69euhUqnw008/ITg4GEZGRggPD9doV1RUBGNjY528p62trU76ISISC0dIiKqZXC6Ho6MjnJ2dMX78eHh5eWHnzp3qaZZFixZBoVDA1dUVAPDnn3/inXfegbW1NWxtbTFgwABcu3ZN3V9JSQlCQ0NhbW0NOzs7fPjhh3jyiRBPTtmoVCpMnz4dDRs2hFwuxyuvvIKvvvoK165dQ48ePQAANjY2kMlkGDFiBACgtLQUkZGRcHFxgampKVq1aoWtW7dqvM9PP/2EZs2awdTUFD169NCIk4jo3zAhIZKYqakpioqKAAAHDhzAhQsXEB8fj927d6O4uBje3t6wsLDAb7/9hiNHjsDc3By9e/dWn/Ppp58iNjYW//3vf3H48GHk5ORoPGG2IsOHD8e3336L6OhonDt3Dp999hnMzc3RsGFDbNu2DcDjJ8feunULK1euBABERkbi66+/RkxMDM6cOYMpU6bgvffew6FDhwA8TpwGDx6Mfv36ITU1FaNHj8aMGTPE+tiISN9I/HA/opdKYGCgMGDAAEEQHj/BOD4+XpDL5cK0adOEwMBAwcHBQVCpVOr2GzduFFxdXYXS0lJ1mUqlEkxNTYWff/5ZEARBqFevnhAVFaWuLy4uFho0aKB+H0EQhG7dugkffPCBIAiCcOHCBQGAEB8fX2GMv/76qwBAuHfvnrqssLBQqF27tnD06FGNtkFBQcK7774rCIIghIeHC+7u7hr106dPL9cXEVFFuIaEqJrt3r0b5ubmKC4uRmlpKYYOHYp58+YhODgYHh4eGutGTp8+jcuXL8PCwkKjj8LCQly5cgV5eXm4desW2rdvr64zNDREu3btyk3blElNTUWtWrXQrVu3Ssd8+fJlPHjwAG+99ZZGeVFREV577TUAwLlz5zTiAABPT89KvwcRvdyYkBBVsx49emDdunUwNjaGQqGAoeH/voZmZmYabQsKCtC2bVts2rSpXD9169Z9pvc3NTWt8jkFBQUAgD179qB+/foadXK5/JniICL6JyYkRNXMzMwMr7zySqXatmnTBps3b4a9vT0sLS0rbFOvXj0cP34cXbt2BQA8evQIKSkpaNOmTYXtPTw8UFpaikOHDsHLy6tcfdkITUlJibrM3d0dcrkcmZmZTx1ZcXNzw86dOzXKjh07pv0iiYjARa1ENVpAQADq1KmDAQMG4LfffkNGRgYOHjyISZMm4caNGwCADz74AB9//DF27NiB8+fPY8KECf96D5FGjRohMDAQo0aNwo4dO9R9fv/99wAAZ2dnyGQy7N69G7dv30ZBQQEsLCwwbdo0TJkyBRs2bMCVK1dw8uRJrFq1Chs2bAAAvP/++7h06RLCwsJw4cIFxMXFITY2VuyPiIj0BBMSohqsdu3aSExMhJOTEwYPHgw3NzcEBQWhsLBQPWIydepUDBs2DIGBgfD09ISFhQUGDRr0r/2uW7cOb7/9NiZMmIDmzZtjzJgxuH//PgCgfv36mD9/PmbMmAEHBweEhIQAAD766CNEREQgMjISbm5u6N27N/bs2QMXFxcAgJOTE7Zt24YdO3agVatWiImJweLFi0X8dIhIn8iEp618IyIiIqomHCEhIiIiyTEhISIiIskxISEiIiLJMSEhIiIiyTEhISIiIskxISEiIiLJMSEhIiIiyTEhISIiIskxISEiIiLJMSEhIiIiyTEhISIiIskxISEiIiLJ/R8YWgCasc4jdwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Classification Report:\")\n",
    "print(classification_report(targets.cpu().numpy(), preds.cpu().numpy()))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(targets.cpu().numpy(), preds.cpu().numpy())\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Plot confusion matrix\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
