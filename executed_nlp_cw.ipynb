{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP CW - Michelle Lo, Hetty Symes, Evelyn Nutton\n",
    "\n",
    "RoBERTa base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /homes/en120/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /homes/en120/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import pipeline, RobertaModel, AutoTokenizer, AutoModelForSequenceClassification, AdamW, DataCollatorWithPadding, get_scheduler\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "import nlpaug.augmenter.word as naw\n",
    "import sacremoses\n",
    "import nltk\n",
    "import math\n",
    "from dataset.dont_patronize_me import DontPatronizeMe\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>par_id</th>\n",
       "      <th>art_id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>country</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_0</td>\n",
       "      <td>@@7258997</td>\n",
       "      <td>vulnerable</td>\n",
       "      <td>us</td>\n",
       "      <td>In the meantime , conservatives are working to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_1</td>\n",
       "      <td>@@16397324</td>\n",
       "      <td>women</td>\n",
       "      <td>pk</td>\n",
       "      <td>In most poor households with no education chil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_2</td>\n",
       "      <td>@@16257812</td>\n",
       "      <td>migrant</td>\n",
       "      <td>ca</td>\n",
       "      <td>The real question is not whether immigration i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_3</td>\n",
       "      <td>@@3509652</td>\n",
       "      <td>migrant</td>\n",
       "      <td>gb</td>\n",
       "      <td>In total , the country 's immigrant population...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t_4</td>\n",
       "      <td>@@477506</td>\n",
       "      <td>vulnerable</td>\n",
       "      <td>ca</td>\n",
       "      <td>Members of the church , which is part of Ken C...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  par_id      art_id     keyword country  \\\n",
       "0    t_0   @@7258997  vulnerable      us   \n",
       "1    t_1  @@16397324       women      pk   \n",
       "2    t_2  @@16257812     migrant      ca   \n",
       "3    t_3   @@3509652     migrant      gb   \n",
       "4    t_4    @@477506  vulnerable      ca   \n",
       "\n",
       "                                                text  \n",
       "0  In the meantime , conservatives are working to...  \n",
       "1  In most poor households with no education chil...  \n",
       "2  The real question is not whether immigration i...  \n",
       "3  In total , the country 's immigrant population...  \n",
       "4  Members of the church , which is part of Ken C...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_df = pd.read_csv(\"train_dev_data/train_set.csv\")\n",
    "test_df = pd.read_csv(\"train_dev_data/dev_set.csv\")\n",
    "dontpatroniseme = DontPatronizeMe(None, \"test_data/task4_test.tsv\")\n",
    "dontpatroniseme.load_test()\n",
    "official_test_df =  dontpatroniseme.test_set_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation via Back Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back_aug = naw.BackTranslationAug(\n",
    "#     from_model_name='Helsinki-NLP/opus-mt-en-de',\n",
    "#     to_model_name='Helsinki-NLP/opus-mt-de-en',\n",
    "#     device='cpu',\n",
    "#     max_length=len(max(train_df[\"text\"].to_list(),key=len))\n",
    "# )\n",
    "# underrep = train_df[train_df['label'] == 1]\n",
    "# underrep_augment = underrep.copy().dropna()\n",
    "# underrep_augment_data_text = []\n",
    "# batch_size_aug = 32\n",
    "# for i in range(0,len(underrep_augment),batch_size_aug):\n",
    "#     underrep_augment_data_text.extend(back_aug.augment(underrep_augment[\"text\"].to_list()[i:i+batch_size_aug]))\n",
    "\n",
    "\n",
    "# print(underrep_augment_data_text)\n",
    "# underrep_augment_data = pd.DataFrame(underrep_augment_data_text)\n",
    "# underrep_augment[\"text\"] = underrep_augment_data\n",
    "\n",
    "\n",
    "# train_df_augment = pd.concat([train_df,underrep_augment.dropna()])\n",
    "\n",
    "# print(train_df['label'].value_counts())\n",
    "# print(train_df_augment['label'].value_counts())\n",
    "\n",
    "# train_df = train_df_augment\n",
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(underrep.head())\n",
    "# print(underrep_augment.tail())\n",
    "# train_df.to_csv(\"train_dev_data/train_set_aug.csv\")\n",
    "\n",
    "# train_df = pd.read_csv(\"train_dev_data/train_set_aug.csv\")\n",
    "\n",
    "# print(train_df['label'].value_counts())\n",
    "\n",
    "# X_train = train_df[['text']]  # Feature columns\n",
    "# y_train = train_df['label']  # Target column\n",
    "\n",
    "# # Initialize the random oversampler\n",
    "# ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "# # Apply oversampling\n",
    "# X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Resampled and Augmented Data into New Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    7581\n",
      "1    7581\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>We 're living in times of absolute insanity , ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>In Libya today , there are countless number of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>\"White House press secretary Sean Spicer said ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Council customers only signs would be displaye...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>\"\"\" Just like we received migrants fleeing El ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  label\n",
       "0           0  We 're living in times of absolute insanity , ...      0\n",
       "1           1  In Libya today , there are countless number of...      0\n",
       "2           2  \"White House press secretary Sean Spicer said ...      0\n",
       "3           3  Council customers only signs would be displaye...      0\n",
       "4           4  \"\"\" Just like we received migrants fleeing El ...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update the dataset with the resampled values\n",
    "# train_df = pd.DataFrame(X_resampled, columns=X_train.columns)\n",
    "# train_df['label'] = y_resampled\n",
    "\n",
    "# train_df.to_csv(\"train_dev_data/train_set_aug_resampled.csv\")\n",
    "\n",
    "\n",
    "# Load pre-sampled, pre-augmented dataset\n",
    "train_df = pd.read_csv(\"train_dev_data/train_set_aug_resampled.csv\")\n",
    "\n",
    "# Verify the oversampling result\n",
    "print(train_df['label'].value_counts())\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Roberta Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model\n",
    "checkpoint = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, truncation=True, do_lower_case=True)\n",
    "pretrained_model = RobertaModel.from_pretrained(checkpoint, num_labels=2)\n",
    "pretrained_model = pretrained_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCLData class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for the data\n",
    "class PCLData(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len, test=False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.text = self.data.text\n",
    "        self.test = test\n",
    "        self.targets = None if test else self.data.label\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor([]) if self.test else torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Dataset: (8375, 7)\n",
      "TEST Dataset: (2094, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAIN Dataset: {}\".format(train_df.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_df.shape))\n",
    "train_dataset = PCLData(train_df, tokenizer, MAX_LEN)\n",
    "test_dataset = PCLData(test_df, tokenizer, MAX_LEN)\n",
    "\n",
    "test_params = {'batch_size': 4, 'shuffle': True, 'num_workers': 0}\n",
    "testing_loader = DataLoader(test_dataset, **test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobertaClass, self).__init__()\n",
    "        self.l1 = pretrained_model\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaClass(\n",
       "  (l1): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RobertaClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Loss - Self Adjusting Dice Loss\n",
    "Taken from the unofficial Pytorch implementation of https://arxiv.org/abs/1911.02855, which can be founds here https://github.com/fursovia/self-adj-dice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "\n",
    "# Taken from the SelfAdjDiceLoss python module source code which cannot be imported regularly due to pytorch compatibility issues\n",
    "class SelfAdjDiceLoss(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    Creates a criterion that optimizes a multi-class Self-adjusting Dice Loss\n",
    "    (\"Dice Loss for Data-imbalanced NLP Tasks\" paper)\n",
    "\n",
    "    Args:\n",
    "        alpha (float): a factor to push down the weight of easy examples\n",
    "        gamma (float): a factor added to both the nominator and the denominator for smoothing purposes\n",
    "        reduction (string): Specifies the reduction to apply to the output:\n",
    "            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
    "            ``'mean'``: the sum of the output will be divided by the number of\n",
    "            elements in the output, ``'sum'``: the output will be summed.\n",
    "\n",
    "    Shape:\n",
    "        - logits: `(N, C)` where `N` is the batch size and `C` is the number of classes.\n",
    "        - targets: `(N)` where each value is in [0, C - 1]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha: float = 1.0, gamma: float = 1.0, reduction: str = \"mean\") -> None:\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        print(logits.shape)\n",
    "        probs = torch.gather(probs, dim=1, index=targets.unsqueeze(1))\n",
    "\n",
    "        probs_with_factor = ((1 - probs) ** self.alpha) * probs\n",
    "        loss = 1 - (2 * probs_with_factor + self.gamma) / (probs_with_factor + 1 + self.gamma)\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        elif self.reduction == \"none\" or self.reduction is None:\n",
    "            return loss\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Reduction `{self.reduction}` is not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating the loss function and optimizer\n",
    "# criterion = SelfAdjDiceLoss()\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.AdamW(params =  model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def calcuate_accuracy(preds, targets):\n",
    "    n_correct = (preds==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, epoch, optimizer, training_loader, scheduler=None):\n",
    "    tr_loss = 0; n_correct = 0; steps = 0; seen = 0\n",
    "    model.train()\n",
    "    for i,data in tqdm(enumerate(training_loader, 0)):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        preds = model(ids, mask, token_type_ids)\n",
    "        loss = loss_function(preds, targets)\n",
    "        tr_loss += loss.item()\n",
    "        _, pred_labels = torch.max(preds.data, dim=1)\n",
    "        n_correct += calcuate_accuracy(pred_labels, targets)\n",
    "\n",
    "        steps += 1\n",
    "        seen+=targets.size(0)\n",
    "        \n",
    "        if i%5000==0:\n",
    "            curr_loss = tr_loss/steps\n",
    "            curr_acc = (n_correct*100)/seen \n",
    "            print(f\"Training Loss per 5000 steps: {curr_loss}\")\n",
    "            print(f\"Training Accuracy per 5000 steps: {curr_acc}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "    print(f'Total Accuracy for Epoch {epoch}: {(n_correct*100)/seen}')\n",
    "    epoch_loss = tr_loss/steps\n",
    "    epoch_accu = (n_correct*100)/seen\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    model.eval()\n",
    "    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; steps=0; seen=0\n",
    "    preds_model = torch.tensor([]).to(device); targets_model = torch.tensor([]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            preds = model(ids, mask, token_type_ids).squeeze()\n",
    "            \n",
    "            _, pred_labels = torch.max(preds.data, dim=1)\n",
    "            n_correct += calcuate_accuracy(pred_labels, targets)\n",
    "\n",
    "            steps += 1\n",
    "            seen+=targets.size(0)\n",
    "\n",
    "            preds_model = torch.cat((preds_model, pred_labels))\n",
    "            targets_model = torch.cat((targets_model, targets))\n",
    "            \n",
    "    epoch_accu = (n_correct*100)/seen\n",
    "\n",
    "    \n",
    "    \n",
    "    return epoch_accu, preds_model, targets_model\n",
    "\n",
    "# acc, preds, targets = valid(model, testing_loader)\n",
    "# print(\"Accuracy on test data = %0.2f%%\" % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_hyperparameters(save_model_name, learning_rate, batch_size, epochs, use_scheduler=False, gamma=0.9):\n",
    "    train_params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 0}\n",
    "    training_loader = DataLoader(train_dataset, **train_params)\n",
    "    model = RobertaClass().to(device)\n",
    "    optimizer = torch.optim.AdamW(params =  model.parameters(), lr=learning_rate)\n",
    "\n",
    "    scheduler = None\n",
    "    if use_scheduler:\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma)\n",
    "    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train(model, epoch, optimizer, training_loader, scheduler)\n",
    "\n",
    "    torch.save(model.state_dict(), f\"models/{save_model_name}.pt\")\n",
    "\n",
    "    acc, preds, targets = valid(model, testing_loader)\n",
    "    print(\"Accuracy on test data = %0.2f%%\" % acc)\n",
    "    print(classification_report(targets.cpu().numpy(), preds.cpu().numpy()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters to tune: learning rate and batch size\n",
    "\n",
    "batch_sizes = [4, 16, 32]\n",
    "learning_rates = [1e-5, 1e-3, 1e-2]\n",
    "gamma_rates = [0.3, 0.5, 0.9]\n",
    "\n",
    "model_id = 0\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for lr in learning_rates:\n",
    "        print(f\"Model {model_id}: Batch size {batch_size}, LR {lr}, no scheduler\")\n",
    "        train_with_hyperparameters(model_id, lr, batch_size, 5, use_scheduler=False)\n",
    "        model_id += 1\n",
    "        for gamma in gamma_rates:\n",
    "            print(f\"Model {model_id}: Batch size {batch_size}, LR {lr}, scheduler with gamma {gamma}\")\n",
    "            train_with_hyperparameters(model_id, lr, batch_size, 5, use_scheduler=True, gamma=gamma)\n",
    "            model_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/en120/dlenv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2681: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "1it [00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.7274256944656372\n",
      "Training Accuracy per 5000 steps: 43.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "948it [33:32,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Accuracy for Epoch 0: 82.1461548608363\n",
      "Training Loss Epoch: 0.4032603351061354\n",
      "Training Accuracy Epoch: 82.1461548608363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.2788306176662445\n",
      "Training Accuracy per 5000 steps: 93.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "948it [27:51,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Accuracy for Epoch 1: 88.93285846194433\n",
      "Training Loss Epoch: 0.2806017319809347\n",
      "Training Accuracy Epoch: 88.93285846194433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.14395150542259216\n",
      "Training Accuracy per 5000 steps: 87.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "948it [34:04,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Accuracy for Epoch 2: 92.53396649518533\n",
      "Training Loss Epoch: 0.2040695380812456\n",
      "Training Accuracy Epoch: 92.53396649518533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.053500596433877945\n",
      "Training Accuracy per 5000 steps: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "948it [23:03,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Accuracy for Epoch 3: 94.11027568922306\n",
      "Training Loss Epoch: 0.17423299776944273\n",
      "Training Accuracy Epoch: 94.11027568922306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.10293321311473846\n",
      "Training Accuracy per 5000 steps: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "948it [17:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Accuracy for Epoch 4: 96.39229653080069\n",
      "Training Loss Epoch: 0.11789965785878714\n",
      "Training Accuracy Epoch: 96.39229653080069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "524it [00:45, 11.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data = 89.83%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.93      0.94      1895\n",
      "         1.0       0.47      0.59      0.53       199\n",
      "\n",
      "    accuracy                           0.90      2094\n",
      "   macro avg       0.71      0.76      0.73      2094\n",
      "weighted avg       0.91      0.90      0.90      2094\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "FINAL_LR = 1e-5\n",
    "FINAL_TRAIN_BATCH_SIZE = 16\n",
    "FINAL_GAMMA = 0.9\n",
    "\n",
    "train_with_hyperparameters(\"final_model\", FINAL_LR, FINAL_TRAIN_BATCH_SIZE, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  par_id      art_id     keyword country  \\\n",
      "0    t_0   @@7258997  vulnerable      us   \n",
      "1    t_1  @@16397324       women      pk   \n",
      "2    t_2  @@16257812     migrant      ca   \n",
      "3    t_3   @@3509652     migrant      gb   \n",
      "4    t_4    @@477506  vulnerable      ca   \n",
      "\n",
      "                                                text  \n",
      "0  In the meantime , conservatives are working to...  \n",
      "1  In most poor households with no education chil...  \n",
      "2  The real question is not whether immigration i...  \n",
      "3  In total , the country 's immigrant population...  \n",
      "4  Members of the church , which is part of Ken C...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/vol/bitbucket/en120/dlenv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2681: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "29it [00:05,  5.13it/s]"
     ]
    }
   ],
   "source": [
    "# print(\"Classification Report:\")\n",
    "# print(classification_report(targets.cpu().numpy(), preds.cpu().numpy()))\n",
    "\n",
    "# # Confusion matrix\n",
    "# cm = confusion_matrix(targets.cpu().numpy(), preds.cpu().numpy())\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(cm)\n",
    "\n",
    "# # Plot confusion matrix\n",
    "# sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"])\n",
    "# plt.xlabel(\"Predicted\")\n",
    "# plt.ylabel(\"Actual\")\n",
    "# plt.title(\"Confusion Matrix\")\n",
    "# plt.show()\n",
    "print(official_test_df.head())\n",
    "\n",
    "def create_eval_txt(dataset_loader, path):\n",
    "  model.eval()\n",
    "  preds_model = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "      for _, data in tqdm(enumerate(dataset_loader, 0)):\n",
    "          ids = data['ids'].to(device, dtype = torch.long)\n",
    "          mask = data['mask'].to(device, dtype = torch.long)\n",
    "          token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "          preds = model(ids, mask, token_type_ids).squeeze()\n",
    "              \n",
    "          _, pred_labels = torch.max(preds.data, dim=1)\n",
    "          preds_model.extend(pred_labels.tolist())\n",
    "  \n",
    "  with open(path, mode='wt', encoding='utf-8') as f:\n",
    "      f.write('\\n'.join([str(p) for p in preds_model]))\n",
    "\n",
    "base_path = \"Results\"\n",
    "create_eval_txt(testing_loader, base_path+\"/dev.txt\")\n",
    "\n",
    "official_test_data = PCLData(official_test_df, tokenizer, MAX_LEN, True)\n",
    "official_test_loader = DataLoader(official_test_data, **test_params)\n",
    "\n",
    "create_eval_txt(official_test_loader, base_path+\"/test.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
